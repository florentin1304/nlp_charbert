{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"imdb\")\n",
    "samples = ds[\"test\"][\"text\"][:100]\n",
    "\n",
    "with open(\"eval_small.txt\", \"a\") as out:\n",
    "    for s in samples:\n",
    "        out.write(s + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/temp/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -r requirements\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-21 20:06:28.387787: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-21 20:06:28.390581: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-21 20:06:28.420941: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-21 20:06:28.420974: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-21 20:06:28.421831: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-21 20:06:28.427396: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-21 20:06:29.216268: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "12/21/2023 20:06:30 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "12/21/2023 20:06:30 - INFO - modeling.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/temp/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "12/21/2023 20:06:30 - INFO - modeling.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "12/21/2023 20:06:31 - INFO - modeling.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/temp/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
      "12/21/2023 20:06:33 - INFO - modeling.modeling_utils -   Weights of CharBertForMaskedLM not initialized from pretrained model: ['bert.char_embeddings.char_embeddings.weight', 'bert.char_embeddings.rnn_layer.weight_ih_l0', 'bert.char_embeddings.rnn_layer.weight_hh_l0', 'bert.char_embeddings.rnn_layer.bias_ih_l0', 'bert.char_embeddings.rnn_layer.bias_hh_l0', 'bert.char_embeddings.rnn_layer.weight_ih_l0_reverse', 'bert.char_embeddings.rnn_layer.weight_hh_l0_reverse', 'bert.char_embeddings.rnn_layer.bias_ih_l0_reverse', 'bert.char_embeddings.rnn_layer.bias_hh_l0_reverse', 'bert.encoder.word_linear1.weight', 'bert.encoder.char_linear1.weight', 'bert.encoder.fusion_layer_list.0.weight', 'bert.encoder.fusion_layer_list.0.bias', 'bert.encoder.fusion_layer_list.1.weight', 'bert.encoder.fusion_layer_list.1.bias', 'bert.encoder.fusion_layer_list.2.weight', 'bert.encoder.fusion_layer_list.2.bias', 'bert.encoder.fusion_layer_list.3.weight', 'bert.encoder.fusion_layer_list.3.bias', 'bert.encoder.fusion_layer_list.4.weight', 'bert.encoder.fusion_layer_list.4.bias', 'bert.encoder.fusion_layer_list.5.weight', 'bert.encoder.fusion_layer_list.5.bias', 'bert.encoder.fusion_layer_list.6.weight', 'bert.encoder.fusion_layer_list.6.bias', 'bert.encoder.fusion_layer_list.7.weight', 'bert.encoder.fusion_layer_list.7.bias', 'bert.encoder.fusion_layer_list.8.weight', 'bert.encoder.fusion_layer_list.8.bias', 'bert.encoder.fusion_layer_list.9.weight', 'bert.encoder.fusion_layer_list.9.bias', 'bert.encoder.fusion_layer_list.10.weight', 'bert.encoder.fusion_layer_list.10.bias', 'bert.encoder.fusion_layer_list.11.weight', 'bert.encoder.fusion_layer_list.11.bias', 'bert.encoder.word_norm.weight', 'bert.encoder.word_norm.bias', 'bert.encoder.char_norm.weight', 'bert.encoder.char_norm.bias', 'cls.predictions.adv_transform.dense.weight', 'cls.predictions.adv_transform.dense.bias', 'cls.predictions.adv_transform.LayerNorm.weight', 'cls.predictions.adv_transform.LayerNorm.bias', 'cls.predictions.adv_decoder.weight', 'cls.predictions.adv_decoder.bias']\n",
      "12/21/2023 20:06:33 - INFO - modeling.modeling_utils -   Weights from pretrained model not used in CharBertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "12/21/2023 20:06:34 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='CharBERT/data/mlm_datasets/train_small.txt', output_dir='model/output/mlm/wiki_20m_eng', eval_data_file='CharBERT/data/mlm_datasets/eval_small.txt', char_vocab='CharBERT/data/dict/bert_char_vocab', term_vocab='CharBERT/data/dict/term_vocab', model_type='bert', model_name_or_path='bert-base-cased', mlm=True, mlm_probability=0.1, adv_probability=0.1, config_name='', data_version='', tokenizer_name='', cache_dir='', block_size=384, do_train=True, do_eval=True, output_debug=False, evaluate_during_training=False, do_lower_case=False, char_maxlen_for_word=6, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=4, gradient_accumulation_steps=1, learning_rate=3e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=10000, input_nraws=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'))\n",
      "/home/temp/anaconda3/envs/flo_nlp/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "12/21/2023 20:06:34 - INFO - __main__ -   ***** Running training *****\n",
      "12/21/2023 20:06:34 - INFO - __main__ -     Num examples = 100\n",
      "12/21/2023 20:06:34 - INFO - __main__ -     Num Epochs = 10\n",
      "12/21/2023 20:06:34 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
      "12/21/2023 20:06:34 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "12/21/2023 20:06:34 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "12/21/2023 20:06:34 - INFO - __main__ -     Total optimization steps = 250\n",
      "cls: <class 'modeling.configuration_bert.BertConfig'>\n",
      "pretrained_model_name_or_path: bert-base-cased\n",
      "pretrained_model_name_or_path: bert-base-cased\n",
      "cls.pretrained_config_archive_map: {'bert-base-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json', 'bert-large-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json', 'bert-base-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json', 'bert-large-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-config.json', 'bert-base-multilingual-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json', 'bert-base-multilingual-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json', 'bert-base-chinese': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json', 'bert-base-german-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-config.json', 'bert-large-uncased-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-config.json', 'bert-large-cased-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-config.json', 'bert-large-uncased-whole-word-masking-finetuned-squad': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-config.json', 'bert-large-cased-whole-word-masking-finetuned-squad': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-config.json', 'bert-base-cased-finetuned-mrpc': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-config.json', 'bert-base-german-dbmdz-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json', 'bert-base-german-dbmdz-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-uncased-config.json', 'bert-base-japanese': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-config.json', 'bert-base-japanese-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking-config.json', 'bert-base-japanese-char': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-char-config.json', 'bert-base-japanese-char-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-char-whole-word-masking-config.json', 'bert-base-finnish-cased-v1': 'https://s3.amazonaws.com/models.huggingface.co/bert/TurkuNLP/bert-base-finnish-cased-v1/config.json', 'bert-base-finnish-uncased-v1': 'https://s3.amazonaws.com/models.huggingface.co/bert/TurkuNLP/bert-base-finnish-uncased-v1/config.json'}\n",
      "Epoch:   0%|                                             | 0/10 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                         | 0/25 [00:00<?, ?it/s]\u001b[A12/21/2023 20:06:34 - INFO - __main__ -   Reading the [1]th data block from dataset file at CharBERT/data/mlm_datasets/train_small.txt\n",
      "\n",
      "Iteration:   4%|█▎                               | 1/25 [00:08<03:35,  8.99s/it]\u001b[A\n",
      "Iteration:   8%|██▋                              | 2/25 [00:09<01:37,  4.26s/it]\u001b[A\n",
      "Iteration:  12%|███▉                             | 3/25 [00:10<01:00,  2.75s/it]\u001b[A\n",
      "Iteration:  16%|█████▎                           | 4/25 [00:11<00:42,  2.04s/it]\u001b[A\n",
      "Iteration:  20%|██████▌                          | 5/25 [00:12<00:33,  1.65s/it]\u001b[A\n",
      "Iteration:  24%|███████▉                         | 6/25 [00:13<00:26,  1.42s/it]\u001b[A\n",
      "Iteration:  28%|█████████▏                       | 7/25 [00:14<00:22,  1.27s/it]\u001b[A\n",
      "Iteration:  32%|██████████▌                      | 8/25 [00:15<00:19,  1.17s/it]\u001b[A\n",
      "Iteration:  36%|███████████▉                     | 9/25 [00:16<00:17,  1.10s/it]\u001b[A\n",
      "Iteration:  40%|████████████▊                   | 10/25 [00:17<00:15,  1.06s/it]\u001b[A\n",
      "Iteration:  44%|██████████████                  | 11/25 [00:18<00:14,  1.03s/it]\u001b[A\n",
      "Iteration:  48%|███████████████▎                | 12/25 [00:19<00:13,  1.01s/it]\u001b[A\n",
      "Iteration:  52%|████████████████▋               | 13/25 [00:20<00:11,  1.01it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▉              | 14/25 [00:21<00:10,  1.02it/s]\u001b[A\n",
      "Iteration:  60%|███████████████████▏            | 15/25 [00:22<00:09,  1.03it/s]\u001b[A\n",
      "Iteration:  64%|████████████████████▍           | 16/25 [00:23<00:08,  1.03it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████▊          | 17/25 [00:24<00:07,  1.03it/s]\u001b[A\n",
      "Iteration:  72%|███████████████████████         | 18/25 [00:25<00:06,  1.04it/s]\u001b[A\n",
      "Iteration:  76%|████████████████████████▎       | 19/25 [00:26<00:05,  1.04it/s]\u001b[A\n",
      "Iteration:  80%|█████████████████████████▌      | 20/25 [00:27<00:04,  1.04it/s]\u001b[A\n",
      "Iteration:  84%|██████████████████████████▉     | 21/25 [00:28<00:03,  1.04it/s]\u001b[A\n",
      "Iteration:  88%|████████████████████████████▏   | 22/25 [00:29<00:02,  1.04it/s]\u001b[A\n",
      "Iteration:  92%|█████████████████████████████▍  | 23/25 [00:30<00:01,  1.04it/s]\u001b[A\n",
      "Iteration:  96%|██████████████████████████████▋ | 24/25 [00:31<00:00,  1.04it/s]\u001b[A\n",
      "Iteration: 100%|████████████████████████████████| 25/25 [00:32<00:00,  1.28s/it]\u001b[A\n",
      "Epoch:  10%|███▋                                 | 1/10 [00:32<04:48, 32.07s/it]\n",
      "Iteration:   0%|                                         | 0/25 [00:00<?, ?it/s]\u001b[A12/21/2023 20:07:06 - INFO - __main__ -   Reading the [1]th data block from dataset file at CharBERT/data/mlm_datasets/train_small.txt\n",
      "\n",
      "Iteration:   4%|█▎                               | 1/25 [00:08<03:25,  8.54s/it]\u001b[A\n",
      "Iteration:   8%|██▋                              | 2/25 [00:09<01:33,  4.09s/it]\u001b[A\n",
      "Iteration:  12%|███▉                             | 3/25 [00:10<00:58,  2.66s/it]\u001b[A\n",
      "Iteration:  16%|█████▎                           | 4/25 [00:11<00:41,  1.99s/it]\u001b[A\n",
      "Iteration:  20%|██████▌                          | 5/25 [00:12<00:32,  1.62s/it]\u001b[A\n",
      "Iteration:  24%|███████▉                         | 6/25 [00:13<00:26,  1.39s/it]\u001b[A\n",
      "Iteration:  28%|█████████▏                       | 7/25 [00:14<00:22,  1.25s/it]\u001b[A\n",
      "Iteration:  32%|██████████▌                      | 8/25 [00:15<00:19,  1.16s/it]\u001b[A\n",
      "Iteration:  36%|███████████▉                     | 9/25 [00:16<00:17,  1.10s/it]\u001b[A\n",
      "Iteration:  40%|████████████▊                   | 10/25 [00:17<00:15,  1.05s/it]\u001b[A\n",
      "Iteration:  44%|██████████████                  | 11/25 [00:18<00:14,  1.02s/it]\u001b[A\n",
      "Iteration:  48%|███████████████▎                | 12/25 [00:19<00:13,  1.00s/it]\u001b[A\n",
      "Iteration:  52%|████████████████▋               | 13/25 [00:20<00:11,  1.01it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▉              | 14/25 [00:21<00:10,  1.02it/s]\u001b[A\n",
      "Iteration:  60%|███████████████████▏            | 15/25 [00:21<00:09,  1.03it/s]\u001b[A\n",
      "Iteration:  64%|████████████████████▍           | 16/25 [00:22<00:08,  1.03it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████▊          | 17/25 [00:23<00:07,  1.03it/s]\u001b[A\n",
      "Iteration:  72%|███████████████████████         | 18/25 [00:24<00:06,  1.03it/s]\u001b[A\n",
      "Iteration:  76%|████████████████████████▎       | 19/25 [00:25<00:05,  1.03it/s]\u001b[A\n",
      "Iteration:  80%|█████████████████████████▌      | 20/25 [00:26<00:04,  1.03it/s]\u001b[A\n",
      "Iteration:  84%|██████████████████████████▉     | 21/25 [00:27<00:03,  1.03it/s]\u001b[A\n",
      "Iteration:  88%|████████████████████████████▏   | 22/25 [00:28<00:02,  1.03it/s]\u001b[A\n",
      "Iteration:  92%|█████████████████████████████▍  | 23/25 [00:29<00:01,  1.04it/s]\u001b[A\n",
      "Iteration:  96%|██████████████████████████████▋ | 24/25 [00:30<00:00,  1.04it/s]\u001b[A/home/temp/anaconda3/envs/flo_nlp/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "\n",
      "Iteration: 100%|████████████████████████████████| 25/25 [00:31<00:00,  1.27s/it]\u001b[A\n",
      "Epoch:  20%|███████▍                             | 2/10 [01:03<04:14, 31.84s/it]\n",
      "Iteration:   0%|                                         | 0/25 [00:00<?, ?it/s]\u001b[A12/21/2023 20:07:38 - INFO - __main__ -   Reading the [1]th data block from dataset file at CharBERT/data/mlm_datasets/train_small.txt\n",
      "\n",
      "Iteration:   4%|█▎                               | 1/25 [00:08<03:28,  8.70s/it]\u001b[A\n",
      "Iteration:   8%|██▋                              | 2/25 [00:09<01:35,  4.15s/it]\u001b[A\n",
      "Iteration:  12%|███▉                             | 3/25 [00:10<00:59,  2.70s/it]\u001b[A\n",
      "Iteration:  16%|█████▎                           | 4/25 [00:11<00:42,  2.01s/it]\u001b[A\n",
      "Iteration:  20%|██████▌                          | 5/25 [00:12<00:32,  1.64s/it]\u001b[A\n",
      "Iteration:  24%|███████▉                         | 6/25 [00:13<00:26,  1.41s/it]\u001b[A\n",
      "Iteration:  28%|█████████▏                       | 7/25 [00:14<00:22,  1.27s/it]\u001b[A\n",
      "Iteration:  32%|██████████▌                      | 8/25 [00:15<00:20,  1.18s/it]\u001b[A\n",
      "Iteration:  36%|███████████▉                     | 9/25 [00:16<00:17,  1.12s/it]\u001b[A\n",
      "Iteration:  40%|████████████▊                   | 10/25 [00:17<00:16,  1.07s/it]\u001b[A\n",
      "Iteration:  44%|██████████████                  | 11/25 [00:18<00:14,  1.04s/it]\u001b[A\n",
      "Iteration:  48%|███████████████▎                | 12/25 [00:19<00:13,  1.02s/it]\u001b[A\n",
      "Iteration:  52%|████████████████▋               | 13/25 [00:20<00:12,  1.01s/it]\u001b[A\n",
      "Iteration:  56%|█████████████████▉              | 14/25 [00:21<00:10,  1.00it/s]\u001b[A\n",
      "Iteration:  60%|███████████████████▏            | 15/25 [00:22<00:09,  1.01it/s]\u001b[A\n",
      "Iteration:  64%|████████████████████▍           | 16/25 [00:23<00:08,  1.01it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████▊          | 17/25 [00:24<00:07,  1.01it/s]\u001b[A\n",
      "Iteration:  72%|███████████████████████         | 18/25 [00:25<00:06,  1.02it/s]\u001b[A\n",
      "Iteration:  76%|████████████████████████▎       | 19/25 [00:26<00:05,  1.02it/s]\u001b[A\n",
      "Iteration:  80%|█████████████████████████▌      | 20/25 [00:27<00:04,  1.02it/s]\u001b[A\n",
      "Iteration:  84%|██████████████████████████▉     | 21/25 [00:28<00:03,  1.02it/s]\u001b[A\n",
      "Iteration:  88%|████████████████████████████▏   | 22/25 [00:29<00:02,  1.02it/s]\u001b[A\n",
      "Iteration:  92%|█████████████████████████████▍  | 23/25 [00:30<00:01,  1.02it/s]\u001b[A\n",
      "Iteration:  96%|██████████████████████████████▋ | 24/25 [00:31<00:00,  1.02it/s]\u001b[A\n",
      "Iteration: 100%|████████████████████████████████| 25/25 [00:32<00:00,  1.29s/it]\u001b[A\n",
      "Epoch:  30%|███████████                          | 3/10 [01:35<03:44, 32.00s/it]\n",
      "Iteration:   0%|                                         | 0/25 [00:00<?, ?it/s]\u001b[A12/21/2023 20:08:10 - INFO - __main__ -   Reading the [1]th data block from dataset file at CharBERT/data/mlm_datasets/train_small.txt\n",
      "\n",
      "Iteration:   4%|█▎                               | 1/25 [00:08<03:30,  8.78s/it]\u001b[A\n",
      "Iteration:   8%|██▋                              | 2/25 [00:09<01:36,  4.20s/it]\u001b[A\n",
      "Iteration:  12%|███▉                             | 3/25 [00:10<01:00,  2.74s/it]\u001b[A\n",
      "Iteration:  16%|█████▎                           | 4/25 [00:11<00:43,  2.05s/it]\u001b[A\n",
      "Iteration:  20%|██████▌                          | 5/25 [00:12<00:33,  1.67s/it]\u001b[A\n",
      "Iteration:  24%|███████▉                         | 6/25 [00:13<00:27,  1.44s/it]\u001b[A\n",
      "Iteration:  28%|█████████▏                       | 7/25 [00:14<00:23,  1.30s/it]\u001b[A\n",
      "Iteration:  32%|██████████▌                      | 8/25 [00:15<00:20,  1.20s/it]\u001b[A\n",
      "Iteration:  36%|███████████▉                     | 9/25 [00:16<00:18,  1.14s/it]\u001b[A\n",
      "Iteration:  40%|████████████▊                   | 10/25 [00:17<00:16,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|██████████████                  | 11/25 [00:18<00:14,  1.07s/it]\u001b[A\n",
      "Iteration:  48%|███████████████▎                | 12/25 [00:19<00:13,  1.05s/it]\u001b[A\n",
      "Iteration:  52%|████████████████▋               | 13/25 [00:20<00:12,  1.03s/it]\u001b[A\n",
      "Iteration:  56%|█████████████████▉              | 14/25 [00:21<00:11,  1.02s/it]\u001b[A\n",
      "Iteration:  60%|███████████████████▏            | 15/25 [00:22<00:10,  1.01s/it]\u001b[A\n",
      "Iteration:  64%|████████████████████▍           | 16/25 [00:23<00:09,  1.01s/it]\u001b[A\n",
      "Iteration:  68%|█████████████████████▊          | 17/25 [00:24<00:08,  1.00s/it]\u001b[A\n",
      "Iteration:  72%|███████████████████████         | 18/25 [00:25<00:07,  1.00s/it]\u001b[A\n",
      "Iteration:  76%|████████████████████████▎       | 19/25 [00:26<00:06,  1.00s/it]\u001b[A\n",
      "Iteration:  80%|█████████████████████████▌      | 20/25 [00:27<00:05,  1.00s/it]\u001b[A\n",
      "Iteration:  84%|██████████████████████████▉     | 21/25 [00:28<00:04,  1.00s/it]\u001b[A\n",
      "Iteration:  88%|████████████████████████████▏   | 22/25 [00:29<00:03,  1.00s/it]\u001b[A\n",
      "Iteration:  92%|█████████████████████████████▍  | 23/25 [00:30<00:02,  1.00s/it]\u001b[A\n",
      "Iteration:  96%|██████████████████████████████▋ | 24/25 [00:31<00:01,  1.00s/it]\u001b[A\n",
      "Iteration: 100%|████████████████████████████████| 25/25 [00:32<00:00,  1.31s/it]\u001b[A\n",
      "Epoch:  40%|██████████████▊                      | 4/10 [02:08<03:13, 32.32s/it]\n",
      "Iteration:   0%|                                         | 0/25 [00:00<?, ?it/s]\u001b[A12/21/2023 20:08:43 - INFO - __main__ -   Reading the [1]th data block from dataset file at CharBERT/data/mlm_datasets/train_small.txt\n",
      "\n",
      "Iteration:   4%|█▎                               | 1/25 [00:08<03:25,  8.58s/it]\u001b[A\n",
      "Iteration:   8%|██▋                              | 2/25 [00:09<01:34,  4.12s/it]\u001b[A\n",
      "Iteration:  12%|███▉                             | 3/25 [00:10<00:59,  2.69s/it]\u001b[A\n",
      "Iteration:  16%|█████▎                           | 4/25 [00:11<00:42,  2.02s/it]\u001b[A\n",
      "Iteration:  20%|██████▌                          | 5/25 [00:12<00:33,  1.65s/it]\u001b[A\n",
      "Iteration:  24%|███████▉                         | 6/25 [00:13<00:27,  1.44s/it]\u001b[A\n",
      "Iteration:  28%|█████████▏                       | 7/25 [00:14<00:23,  1.29s/it]\u001b[A\n",
      "Iteration:  32%|██████████▌                      | 8/25 [00:15<00:20,  1.20s/it]\u001b[A\n",
      "Iteration:  36%|███████████▉                     | 9/25 [00:16<00:18,  1.14s/it]\u001b[A\n",
      "Iteration:  40%|████████████▊                   | 10/25 [00:17<00:16,  1.10s/it]\u001b[A\n",
      "Iteration:  44%|██████████████                  | 11/25 [00:18<00:14,  1.07s/it]\u001b[A\n",
      "Iteration:  48%|███████████████▎                | 12/25 [00:19<00:13,  1.05s/it]\u001b[A\n",
      "Iteration:  52%|████████████████▋               | 13/25 [00:20<00:12,  1.03s/it]\u001b[A\n",
      "Iteration:  56%|█████████████████▉              | 14/25 [00:21<00:11,  1.03s/it]\u001b[A\n",
      "Iteration:  60%|███████████████████▏            | 15/25 [00:22<00:10,  1.02s/it]\u001b[A\n",
      "Iteration:  64%|████████████████████▍           | 16/25 [00:23<00:09,  1.01s/it]\u001b[A\n",
      "Iteration:  68%|█████████████████████▊          | 17/25 [00:24<00:08,  1.01s/it]\u001b[A\n",
      "Iteration:  72%|███████████████████████         | 18/25 [00:25<00:07,  1.01s/it]\u001b[A\n",
      "Iteration:  76%|████████████████████████▎       | 19/25 [00:26<00:06,  1.01s/it]\u001b[A\n",
      "Iteration:  80%|█████████████████████████▌      | 20/25 [00:27<00:05,  1.01s/it]\u001b[A\n",
      "Iteration:  84%|██████████████████████████▉     | 21/25 [00:28<00:04,  1.01s/it]\u001b[A\n",
      "Iteration:  88%|████████████████████████████▏   | 22/25 [00:29<00:03,  1.01s/it]\u001b[A\n",
      "Iteration:  92%|█████████████████████████████▍  | 23/25 [00:30<00:02,  1.01s/it]\u001b[A\n",
      "Iteration:  96%|██████████████████████████████▋ | 24/25 [00:31<00:01,  1.01s/it]\u001b[A\n",
      "Iteration: 100%|████████████████████████████████| 25/25 [00:32<00:00,  1.31s/it]\u001b[A\n",
      "Epoch:  50%|██████████████████▌                  | 5/10 [02:41<02:42, 32.47s/it]\n",
      "Iteration:   0%|                                         | 0/25 [00:00<?, ?it/s]\u001b[A12/21/2023 20:09:16 - INFO - __main__ -   Reading the [1]th data block from dataset file at CharBERT/data/mlm_datasets/train_small.txt\n",
      "\n",
      "Iteration:   4%|█▎                               | 1/25 [00:08<03:26,  8.62s/it]\u001b[A\n",
      "Iteration:   8%|██▋                              | 2/25 [00:09<01:35,  4.14s/it]\u001b[A\n",
      "Iteration:  12%|███▉                             | 3/25 [00:10<00:59,  2.71s/it]\u001b[A\n",
      "Iteration:  16%|█████▎                           | 4/25 [00:11<00:42,  2.04s/it]\u001b[A\n",
      "Iteration:  20%|██████▌                          | 5/25 [00:12<00:33,  1.66s/it]\u001b[A\n",
      "Iteration:  24%|███████▉                         | 6/25 [00:13<00:27,  1.43s/it]\u001b[A\n",
      "Iteration:  28%|█████████▏                       | 7/25 [00:14<00:23,  1.29s/it]\u001b[A\n",
      "Iteration:  32%|██████████▌                      | 8/25 [00:15<00:20,  1.20s/it]\u001b[A\n",
      "Iteration:  36%|███████████▉                     | 9/25 [00:16<00:18,  1.14s/it]\u001b[A\n",
      "Iteration:  40%|████████████▊                   | 10/25 [00:17<00:16,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|██████████████                  | 11/25 [00:18<00:14,  1.06s/it]\u001b[A\n",
      "Iteration:  48%|███████████████▎                | 12/25 [00:19<00:13,  1.04s/it]\u001b[A\n",
      "Iteration:  52%|████████████████▋               | 13/25 [00:20<00:12,  1.03s/it]\u001b[A\n",
      "Iteration:  56%|█████████████████▉              | 14/25 [00:21<00:11,  1.02s/it]\u001b[A\n",
      "Iteration:  60%|███████████████████▏            | 15/25 [00:22<00:10,  1.01s/it]\u001b[A\n",
      "Iteration:  64%|████████████████████▍           | 16/25 [00:23<00:09,  1.01s/it]\u001b[A\n",
      "Iteration:  68%|█████████████████████▊          | 17/25 [00:24<00:08,  1.00s/it]\u001b[A\n",
      "Iteration:  72%|███████████████████████         | 18/25 [00:25<00:06,  1.00it/s]\u001b[A\n",
      "Iteration:  76%|████████████████████████▎       | 19/25 [00:26<00:06,  1.00s/it]\u001b[A\n",
      "Iteration:  80%|█████████████████████████▌      | 20/25 [00:27<00:04,  1.00it/s]\u001b[A\n",
      "Iteration:  84%|██████████████████████████▉     | 21/25 [00:28<00:03,  1.00it/s]\u001b[A\n",
      "Iteration:  88%|████████████████████████████▏   | 22/25 [00:29<00:03,  1.00s/it]\u001b[A\n",
      "Iteration:  92%|█████████████████████████████▍  | 23/25 [00:30<00:02,  1.00s/it]\u001b[A\n",
      "Iteration:  96%|██████████████████████████████▋ | 24/25 [00:31<00:00,  1.00it/s]\u001b[A\n",
      "Iteration: 100%|████████████████████████████████| 25/25 [00:32<00:00,  1.30s/it]\u001b[A\n",
      "Epoch:  60%|██████████████████████▏              | 6/10 [03:14<02:10, 32.51s/it]\n",
      "Iteration:   0%|                                         | 0/25 [00:00<?, ?it/s]\u001b[A12/21/2023 20:09:48 - INFO - __main__ -   Reading the [1]th data block from dataset file at CharBERT/data/mlm_datasets/train_small.txt\n",
      "\n",
      "Iteration:   4%|█▎                               | 1/25 [00:08<03:30,  8.79s/it]\u001b[A\n",
      "Iteration:   8%|██▋                              | 2/25 [00:09<01:36,  4.20s/it]\u001b[A\n",
      "Iteration:  12%|███▉                             | 3/25 [00:10<01:00,  2.74s/it]\u001b[A\n",
      "Iteration:  16%|█████▎                           | 4/25 [00:11<00:43,  2.05s/it]\u001b[A\n",
      "Iteration:  20%|██████▌                          | 5/25 [00:12<00:33,  1.67s/it]\u001b[A\n",
      "Iteration:  24%|███████▉                         | 6/25 [00:13<00:27,  1.45s/it]\u001b[A\n",
      "Iteration:  28%|█████████▏                       | 7/25 [00:14<00:23,  1.30s/it]\u001b[A\n",
      "Iteration:  32%|██████████▌                      | 8/25 [00:15<00:20,  1.21s/it]\u001b[A\n",
      "Iteration:  36%|███████████▉                     | 9/25 [00:16<00:18,  1.14s/it]\u001b[A\n",
      "Iteration:  40%|████████████▊                   | 10/25 [00:17<00:16,  1.10s/it]\u001b[A\n",
      "Iteration:  44%|██████████████                  | 11/25 [00:18<00:14,  1.07s/it]\u001b[A\n",
      "Iteration:  48%|███████████████▎                | 12/25 [00:19<00:13,  1.05s/it]\u001b[A\n",
      "Iteration:  52%|████████████████▋               | 13/25 [00:20<00:12,  1.03s/it]\u001b[A\n",
      "Iteration:  56%|█████████████████▉              | 14/25 [00:21<00:11,  1.03s/it]\u001b[A\n",
      "Iteration:  60%|███████████████████▏            | 15/25 [00:22<00:10,  1.02s/it]\u001b[A\n",
      "Iteration:  64%|████████████████████▍           | 16/25 [00:23<00:09,  1.02s/it]\u001b[A\n",
      "Iteration:  68%|█████████████████████▊          | 17/25 [00:24<00:08,  1.01s/it]\u001b[A\n",
      "Iteration:  72%|███████████████████████         | 18/25 [00:25<00:07,  1.01s/it]\u001b[A\n",
      "Iteration:  76%|████████████████████████▎       | 19/25 [00:26<00:06,  1.01s/it]\u001b[A\n",
      "Iteration:  80%|█████████████████████████▌      | 20/25 [00:27<00:05,  1.01s/it]\u001b[A\n",
      "Iteration:  84%|██████████████████████████▉     | 21/25 [00:28<00:04,  1.02s/it]\u001b[A\n",
      "Iteration:  88%|████████████████████████████▏   | 22/25 [00:29<00:03,  1.02s/it]\u001b[A\n",
      "Iteration:  92%|█████████████████████████████▍  | 23/25 [00:30<00:02,  1.02s/it]\u001b[A\n",
      "Iteration:  96%|██████████████████████████████▋ | 24/25 [00:31<00:01,  1.02s/it]\u001b[A\n",
      "Iteration: 100%|████████████████████████████████| 25/25 [00:32<00:00,  1.32s/it]\u001b[A\n",
      "Epoch:  70%|█████████████████████████▉           | 7/10 [03:47<01:38, 32.67s/it]\n",
      "Iteration:   0%|                                         | 0/25 [00:00<?, ?it/s]\u001b[A12/21/2023 20:10:21 - INFO - __main__ -   Reading the [1]th data block from dataset file at CharBERT/data/mlm_datasets/train_small.txt\n",
      "\n",
      "Iteration:   4%|█▎                               | 1/25 [00:08<03:27,  8.65s/it]\u001b[A\n",
      "Iteration:   8%|██▋                              | 2/25 [00:09<01:35,  4.15s/it]\u001b[A\n",
      "Iteration:  12%|███▉                             | 3/25 [00:10<00:59,  2.72s/it]\u001b[A\n",
      "Iteration:  16%|█████▎                           | 4/25 [00:11<00:42,  2.04s/it]\u001b[A\n",
      "Iteration:  20%|██████▌                          | 5/25 [00:12<00:33,  1.67s/it]\u001b[A\n",
      "Iteration:  24%|███████▉                         | 6/25 [00:13<00:27,  1.45s/it]\u001b[A\n",
      "Iteration:  28%|█████████▏                       | 7/25 [00:14<00:23,  1.30s/it]\u001b[A\n",
      "Iteration:  32%|██████████▌                      | 8/25 [00:15<00:20,  1.21s/it]\u001b[A\n",
      "Iteration:  36%|███████████▉                     | 9/25 [00:16<00:18,  1.15s/it]\u001b[A\n",
      "Iteration:  40%|████████████▊                   | 10/25 [00:17<00:16,  1.11s/it]\u001b[A\n",
      "Iteration:  44%|██████████████                  | 11/25 [00:18<00:15,  1.08s/it]\u001b[A\n",
      "Iteration:  48%|███████████████▎                | 12/25 [00:19<00:13,  1.05s/it]\u001b[A\n",
      "Iteration:  52%|████████████████▋               | 13/25 [00:20<00:12,  1.03s/it]\u001b[A\n",
      "Iteration:  56%|█████████████████▉              | 14/25 [00:21<00:11,  1.02s/it]\u001b[A\n",
      "Iteration:  60%|███████████████████▏            | 15/25 [00:22<00:10,  1.02s/it]\u001b[A\n",
      "Iteration:  64%|████████████████████▍           | 16/25 [00:23<00:09,  1.01s/it]\u001b[A\n",
      "Iteration:  68%|█████████████████████▊          | 17/25 [00:24<00:08,  1.01s/it]\u001b[A\n",
      "Iteration:  72%|███████████████████████         | 18/25 [00:25<00:07,  1.00s/it]\u001b[A\n",
      "Iteration:  76%|████████████████████████▎       | 19/25 [00:26<00:06,  1.00s/it]\u001b[A\n",
      "Iteration:  80%|█████████████████████████▌      | 20/25 [00:27<00:04,  1.00it/s]\u001b[A\n",
      "Iteration:  84%|██████████████████████████▉     | 21/25 [00:28<00:03,  1.00it/s]\u001b[A\n",
      "Iteration:  88%|████████████████████████████▏   | 22/25 [00:29<00:02,  1.01it/s]\u001b[A\n",
      "Iteration:  92%|█████████████████████████████▍  | 23/25 [00:30<00:01,  1.01it/s]\u001b[A\n",
      "Iteration:  96%|██████████████████████████████▋ | 24/25 [00:31<00:00,  1.01it/s]\u001b[A\n",
      "Iteration: 100%|████████████████████████████████| 25/25 [00:32<00:00,  1.31s/it]\u001b[A\n",
      "Epoch:  80%|█████████████████████████████▌       | 8/10 [04:19<01:05, 32.68s/it]\n",
      "Iteration:   0%|                                         | 0/25 [00:00<?, ?it/s]\u001b[A12/21/2023 20:10:54 - INFO - __main__ -   Reading the [1]th data block from dataset file at CharBERT/data/mlm_datasets/train_small.txt\n",
      "\n",
      "Iteration:   4%|█▎                               | 1/25 [00:08<03:30,  8.75s/it]\u001b[A\n",
      "Iteration:   8%|██▋                              | 2/25 [00:09<01:36,  4.19s/it]\u001b[A\n",
      "Iteration:  12%|███▉                             | 3/25 [00:10<01:00,  2.73s/it]\u001b[A\n",
      "Iteration:  16%|█████▎                           | 4/25 [00:11<00:42,  2.04s/it]\u001b[A\n",
      "Iteration:  20%|██████▌                          | 5/25 [00:12<00:33,  1.67s/it]\u001b[A\n",
      "Iteration:  24%|███████▉                         | 6/25 [00:13<00:27,  1.44s/it]\u001b[A\n",
      "Iteration:  28%|█████████▏                       | 7/25 [00:14<00:23,  1.29s/it]\u001b[A\n",
      "Iteration:  32%|██████████▌                      | 8/25 [00:15<00:20,  1.20s/it]\u001b[A\n",
      "Iteration:  36%|███████████▉                     | 9/25 [00:16<00:18,  1.14s/it]\u001b[A\n",
      "Iteration:  40%|████████████▊                   | 10/25 [00:17<00:16,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|██████████████                  | 11/25 [00:18<00:14,  1.06s/it]\u001b[A\n",
      "Iteration:  48%|███████████████▎                | 12/25 [00:19<00:13,  1.04s/it]\u001b[A\n",
      "Iteration:  52%|████████████████▋               | 13/25 [00:20<00:12,  1.03s/it]\u001b[A\n",
      "Iteration:  56%|█████████████████▉              | 14/25 [00:21<00:11,  1.02s/it]\u001b[A\n",
      "Iteration:  60%|███████████████████▏            | 15/25 [00:22<00:10,  1.01s/it]\u001b[A\n",
      "Iteration:  64%|████████████████████▍           | 16/25 [00:23<00:09,  1.01s/it]\u001b[A\n",
      "Iteration:  68%|█████████████████████▊          | 17/25 [00:24<00:08,  1.01s/it]\u001b[A\n",
      "Iteration:  72%|███████████████████████         | 18/25 [00:25<00:07,  1.00s/it]\u001b[A\n",
      "Iteration:  76%|████████████████████████▎       | 19/25 [00:26<00:06,  1.00s/it]\u001b[A\n",
      "Iteration:  80%|█████████████████████████▌      | 20/25 [00:27<00:04,  1.00it/s]\u001b[A\n",
      "Iteration:  84%|██████████████████████████▉     | 21/25 [00:28<00:03,  1.00it/s]\u001b[A\n",
      "Iteration:  88%|████████████████████████████▏   | 22/25 [00:29<00:02,  1.00it/s]\u001b[A\n",
      "Iteration:  92%|█████████████████████████████▍  | 23/25 [00:30<00:01,  1.00it/s]\u001b[A\n",
      "Iteration:  96%|██████████████████████████████▋ | 24/25 [00:31<00:00,  1.00it/s]\u001b[A\n",
      "Iteration: 100%|████████████████████████████████| 25/25 [00:32<00:00,  1.31s/it]\u001b[A\n",
      "Epoch:  90%|█████████████████████████████████▎   | 9/10 [04:52<00:32, 32.69s/it]\n",
      "Iteration:   0%|                                         | 0/25 [00:00<?, ?it/s]\u001b[A12/21/2023 20:11:27 - INFO - __main__ -   Reading the [1]th data block from dataset file at CharBERT/data/mlm_datasets/train_small.txt\n",
      "\n",
      "Iteration:   4%|█▎                               | 1/25 [00:08<03:30,  8.78s/it]\u001b[A\n",
      "Iteration:   8%|██▋                              | 2/25 [00:09<01:36,  4.19s/it]\u001b[A\n",
      "Iteration:  12%|███▉                             | 3/25 [00:10<00:59,  2.73s/it]\u001b[A\n",
      "Iteration:  16%|█████▎                           | 4/25 [00:11<00:42,  2.04s/it]\u001b[A\n",
      "Iteration:  20%|██████▌                          | 5/25 [00:12<00:33,  1.66s/it]\u001b[A\n",
      "Iteration:  24%|███████▉                         | 6/25 [00:13<00:27,  1.43s/it]\u001b[A\n",
      "Iteration:  28%|█████████▏                       | 7/25 [00:14<00:23,  1.29s/it]\u001b[A\n",
      "Iteration:  32%|██████████▌                      | 8/25 [00:15<00:20,  1.19s/it]\u001b[A\n",
      "Iteration:  36%|███████████▉                     | 9/25 [00:16<00:18,  1.13s/it]\u001b[A\n",
      "Iteration:  40%|████████████▊                   | 10/25 [00:17<00:16,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|██████████████                  | 11/25 [00:18<00:14,  1.06s/it]\u001b[A\n",
      "Iteration:  48%|███████████████▎                | 12/25 [00:19<00:13,  1.04s/it]\u001b[A\n",
      "Iteration:  52%|████████████████▋               | 13/25 [00:20<00:12,  1.02s/it]\u001b[A\n",
      "Iteration:  56%|█████████████████▉              | 14/25 [00:21<00:11,  1.01s/it]\u001b[A\n",
      "Iteration:  60%|███████████████████▏            | 15/25 [00:22<00:10,  1.00s/it]\u001b[A\n",
      "Iteration:  64%|████████████████████▍           | 16/25 [00:23<00:08,  1.00it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████▊          | 17/25 [00:24<00:07,  1.00it/s]\u001b[A\n",
      "Iteration:  72%|███████████████████████         | 18/25 [00:25<00:06,  1.01it/s]\u001b[A\n",
      "Iteration:  76%|████████████████████████▎       | 19/25 [00:26<00:05,  1.01it/s]\u001b[A\n",
      "Iteration:  80%|█████████████████████████▌      | 20/25 [00:27<00:04,  1.01it/s]\u001b[A\n",
      "Iteration:  84%|██████████████████████████▉     | 21/25 [00:28<00:03,  1.01it/s]\u001b[A\n",
      "Iteration:  88%|████████████████████████████▏   | 22/25 [00:29<00:02,  1.00it/s]\u001b[A\n",
      "Iteration:  92%|█████████████████████████████▍  | 23/25 [00:30<00:01,  1.00it/s]\u001b[A\n",
      "Iteration:  96%|██████████████████████████████▋ | 24/25 [00:31<00:00,  1.00it/s]\u001b[A\n",
      "Iteration: 100%|████████████████████████████████| 25/25 [00:32<00:00,  1.30s/it]\u001b[A\n",
      "Epoch: 100%|████████████████████████████████████| 10/10 [05:25<00:00, 32.51s/it]\n",
      "12/21/2023 20:11:59 - INFO - __main__ -    global_step = 250, average loss = 12.994447883605957\n",
      "12/21/2023 20:11:59 - INFO - __main__ -   Saving model checkpoint to model/output/mlm/wiki_20m_eng\n",
      "12/21/2023 20:11:59 - INFO - modeling.configuration_utils -   Configuration saved in model/output/mlm/wiki_20m_eng/config.json\n",
      "12/21/2023 20:12:00 - INFO - modeling.modeling_utils -   Model weights saved in model/output/mlm/wiki_20m_eng/pytorch_model.bin\n",
      "12/21/2023 20:12:00 - INFO - modeling.configuration_utils -   loading configuration file model/output/mlm/wiki_20m_eng/config.json\n",
      "12/21/2023 20:12:00 - INFO - modeling.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "12/21/2023 20:12:00 - INFO - modeling.modeling_utils -   loading weights file model/output/mlm/wiki_20m_eng/pytorch_model.bin\n",
      "12/21/2023 20:12:02 - INFO - __main__ -   Evaluate the following checkpoints: ['model/output/mlm/wiki_20m_eng']\n",
      "12/21/2023 20:12:02 - INFO - modeling.configuration_utils -   loading configuration file model/output/mlm/wiki_20m_eng/config.json\n",
      "12/21/2023 20:12:02 - INFO - modeling.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "12/21/2023 20:12:02 - INFO - modeling.modeling_utils -   loading weights file model/output/mlm/wiki_20m_eng/pytorch_model.bin\n",
      "12/21/2023 20:12:05 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/21/2023 20:12:05 - INFO - __main__ -     Num examples = 100\n",
      "12/21/2023 20:12:05 - INFO - __main__ -     Batch size = 4\n",
      "cls: <class 'modeling.configuration_bert.BertConfig'>\n",
      "pretrained_model_name_or_path: model/output/mlm/wiki_20m_eng\n",
      "pretrained_model_name_or_path: model/output/mlm/wiki_20m_eng\n",
      "cls.pretrained_config_archive_map: {'bert-base-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json', 'bert-large-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json', 'bert-base-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json', 'bert-large-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-config.json', 'bert-base-multilingual-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json', 'bert-base-multilingual-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json', 'bert-base-chinese': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json', 'bert-base-german-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-config.json', 'bert-large-uncased-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-config.json', 'bert-large-cased-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-config.json', 'bert-large-uncased-whole-word-masking-finetuned-squad': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-config.json', 'bert-large-cased-whole-word-masking-finetuned-squad': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-config.json', 'bert-base-cased-finetuned-mrpc': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-config.json', 'bert-base-german-dbmdz-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json', 'bert-base-german-dbmdz-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-uncased-config.json', 'bert-base-japanese': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-config.json', 'bert-base-japanese-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking-config.json', 'bert-base-japanese-char': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-char-config.json', 'bert-base-japanese-char-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-char-whole-word-masking-config.json', 'bert-base-finnish-cased-v1': 'https://s3.amazonaws.com/models.huggingface.co/bert/TurkuNLP/bert-base-finnish-cased-v1/config.json', 'bert-base-finnish-uncased-v1': 'https://s3.amazonaws.com/models.huggingface.co/bert/TurkuNLP/bert-base-finnish-uncased-v1/config.json'}\n",
      "cls: <class 'modeling.configuration_bert.BertConfig'>\n",
      "pretrained_model_name_or_path: model/output/mlm/wiki_20m_eng\n",
      "pretrained_model_name_or_path: model/output/mlm/wiki_20m_eng\n",
      "cls.pretrained_config_archive_map: {'bert-base-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json', 'bert-large-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json', 'bert-base-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json', 'bert-large-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-config.json', 'bert-base-multilingual-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json', 'bert-base-multilingual-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json', 'bert-base-chinese': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json', 'bert-base-german-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-config.json', 'bert-large-uncased-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-config.json', 'bert-large-cased-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-config.json', 'bert-large-uncased-whole-word-masking-finetuned-squad': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-config.json', 'bert-large-cased-whole-word-masking-finetuned-squad': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-config.json', 'bert-base-cased-finetuned-mrpc': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-config.json', 'bert-base-german-dbmdz-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json', 'bert-base-german-dbmdz-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-uncased-config.json', 'bert-base-japanese': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-config.json', 'bert-base-japanese-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking-config.json', 'bert-base-japanese-char': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-char-config.json', 'bert-base-japanese-char-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-char-whole-word-masking-config.json', 'bert-base-finnish-cased-v1': 'https://s3.amazonaws.com/models.huggingface.co/bert/TurkuNLP/bert-base-finnish-cased-v1/config.json', 'bert-base-finnish-uncased-v1': 'https://s3.amazonaws.com/models.huggingface.co/bert/TurkuNLP/bert-base-finnish-uncased-v1/config.json'}\n",
      "Evaluating:   0%|                                        | 0/25 [00:00<?, ?it/s]12/21/2023 20:12:05 - INFO - __main__ -   Reading the [1]th data block from dataset file at CharBERT/data/mlm_datasets/eval_small.txt\n",
      "Evaluating: 100%|███████████████████████████████| 25/25 [00:23<00:00,  1.08it/s]\n",
      "12/21/2023 20:12:28 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/21/2023 20:12:28 - INFO - __main__ -     adv_all_num = 7438\n",
      "12/21/2023 20:12:28 - INFO - __main__ -     adv_hit_at_1 = 0.1332347405216456\n",
      "12/21/2023 20:12:28 - INFO - __main__ -     adv_hit_at_5 = 0.2549072331271847\n",
      "12/21/2023 20:12:28 - INFO - __main__ -     perplexity = tensor(51.3894)\n"
     ]
    }
   ],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=1,2,3\n",
    "\n",
    "#pretrain charbert by bert_base_cased model\n",
    "#--char_vocab ./data/dict/roberta_char_vocab for robera\n",
    "\n",
    "!python3 CharBERT/run_lm_finetuning.py \\\n",
    "    --model_type bert \\\n",
    "    --model_name_or_path bert-base-cased \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_data_file CharBERT/data/mlm_datasets/train_small.txt \\\n",
    "    --eval_data_file  CharBERT/data/mlm_datasets/eval_small.txt \\\n",
    "    --term_vocab CharBERT/data/dict/term_vocab \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --char_vocab CharBERT/data/dict/bert_char_vocab \\\n",
    "    --mlm_probability 0.10 \\\n",
    "    --input_nraws 1000 \\\n",
    "    --per_gpu_train_batch_size 4 \\\n",
    "    --per_gpu_eval_batch_size 4 \\\n",
    "    --save_steps 10000 \\\n",
    "    --block_size 384 \\\n",
    "    --mlm \\\n",
    "    --overwrite_output_dir \\\n",
    "    --output_dir model/output/mlm/wiki_20m_eng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flo_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
