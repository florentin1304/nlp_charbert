{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "\n",
    "#from datasets import load_dataset\n",
    "\n",
    "#ds = load_dataset(\"imdb\")\n",
    "#samples = ds[\"test\"][\"text\"][:100]\n",
    "\n",
    "#with open(\"eval_small.txt\", \"a\") as out:\n",
    "#    for s in samples:\n",
    "#        out.write(s + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements\n",
    "#import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python datasets/load_general_dataset.py\n",
    "#! python datasets/load_multilingual_dataset.py\n",
    "! python datasets/load_medical_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Shift - Medical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain - RUN MLM \n",
    "\n",
    "BertCheckpoint: bert-base-cased\n",
    "\n",
    "Dataset: PubMed\n",
    "\n",
    "Output Dir: model/output/medical/mlm/base_bert_on_pubmed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-13 17:14:13.938095: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-13 17:14:13.940678: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-13 17:14:13.970460: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-13 17:14:13.970495: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-13 17:14:13.971360: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-13 17:14:13.976384: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-13 17:14:14.772061: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "02/13/2024 17:14:15 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "02/13/2024 17:14:16 - INFO - modeling.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmp6qrl8r71\n",
      "02/13/2024 17:14:16 - INFO - modeling.file_utils -   copying /tmp/tmp6qrl8r71 to cache at /home/temp/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "02/13/2024 17:14:16 - INFO - modeling.file_utils -   creating metadata file for /home/temp/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "02/13/2024 17:14:16 - INFO - modeling.file_utils -   removing temp file /tmp/tmp6qrl8r71\n",
      "02/13/2024 17:14:16 - INFO - modeling.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/temp/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "02/13/2024 17:14:16 - INFO - modeling.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "QUI\n",
      "device DEVICE DEVICE DEVICE DEVICE: cuda\n",
      "cls: <class 'modeling.configuration_bert.BertConfig'>\n",
      "pretrained_model_name_or_path: bert-base-uncased\n",
      "pretrained_model_name_or_path: bert-base-uncased\n",
      "cls.pretrained_config_archive_map: {'bert-base-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json', 'bert-large-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json', 'bert-base-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json', 'bert-large-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-config.json', 'bert-base-multilingual-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json', 'bert-base-multilingual-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json', 'bert-base-chinese': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json', 'bert-base-german-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-config.json', 'bert-large-uncased-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-config.json', 'bert-large-cased-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-config.json', 'bert-large-uncased-whole-word-masking-finetuned-squad': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-config.json', 'bert-large-cased-whole-word-masking-finetuned-squad': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-config.json', 'bert-base-cased-finetuned-mrpc': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-config.json', 'bert-base-german-dbmdz-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json', 'bert-base-german-dbmdz-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-uncased-config.json', 'bert-base-japanese': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-config.json', 'bert-base-japanese-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking-config.json', 'bert-base-japanese-char': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-char-config.json', 'bert-base-japanese-char-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-char-whole-word-masking-config.json', 'bert-base-finnish-cased-v1': 'https://s3.amazonaws.com/models.huggingface.co/bert/TurkuNLP/bert-base-finnish-cased-v1/config.json', 'bert-base-finnish-uncased-v1': 'https://s3.amazonaws.com/models.huggingface.co/bert/TurkuNLP/bert-base-finnish-uncased-v1/config.json'}\n",
      "tokenizer_config.json: 100%|██████████████████| 28.0/28.0 [00:00<00:00, 157kB/s]\n",
      "vocab.txt: 100%|█████████████████████████████| 232k/232k [00:00<00:00, 1.12MB/s]\n",
      "tokenizer.json: 100%|████████████████████████| 466k/466k [00:00<00:00, 4.49MB/s]\n",
      "config.json: 100%|█████████████████████████████| 570/570 [00:00<00:00, 3.54MB/s]\n",
      "02/13/2024 17:14:19 - INFO - modeling.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmp_u35mrs9\n",
      "02/13/2024 17:15:02 - INFO - modeling.file_utils -   copying /tmp/tmp_u35mrs9 to cache at /home/temp/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "02/13/2024 17:15:02 - INFO - modeling.file_utils -   creating metadata file for /home/temp/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "02/13/2024 17:15:02 - INFO - modeling.file_utils -   removing temp file /tmp/tmp_u35mrs9\n",
      "02/13/2024 17:15:02 - INFO - modeling.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/temp/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "02/13/2024 17:15:05 - INFO - modeling.modeling_utils -   Weights of CharBertForMaskedLM not initialized from pretrained model: ['bert.char_embeddings.char_embeddings.weight', 'bert.char_embeddings.rnn_layer.weight_ih_l0', 'bert.char_embeddings.rnn_layer.weight_hh_l0', 'bert.char_embeddings.rnn_layer.bias_ih_l0', 'bert.char_embeddings.rnn_layer.bias_hh_l0', 'bert.char_embeddings.rnn_layer.weight_ih_l0_reverse', 'bert.char_embeddings.rnn_layer.weight_hh_l0_reverse', 'bert.char_embeddings.rnn_layer.bias_ih_l0_reverse', 'bert.char_embeddings.rnn_layer.bias_hh_l0_reverse', 'bert.encoder.word_linear1.weight', 'bert.encoder.char_linear1.weight', 'bert.encoder.fusion_layer_list.0.weight', 'bert.encoder.fusion_layer_list.0.bias', 'bert.encoder.fusion_layer_list.1.weight', 'bert.encoder.fusion_layer_list.1.bias', 'bert.encoder.fusion_layer_list.2.weight', 'bert.encoder.fusion_layer_list.2.bias', 'bert.encoder.fusion_layer_list.3.weight', 'bert.encoder.fusion_layer_list.3.bias', 'bert.encoder.fusion_layer_list.4.weight', 'bert.encoder.fusion_layer_list.4.bias', 'bert.encoder.fusion_layer_list.5.weight', 'bert.encoder.fusion_layer_list.5.bias', 'bert.encoder.fusion_layer_list.6.weight', 'bert.encoder.fusion_layer_list.6.bias', 'bert.encoder.fusion_layer_list.7.weight', 'bert.encoder.fusion_layer_list.7.bias', 'bert.encoder.fusion_layer_list.8.weight', 'bert.encoder.fusion_layer_list.8.bias', 'bert.encoder.fusion_layer_list.9.weight', 'bert.encoder.fusion_layer_list.9.bias', 'bert.encoder.fusion_layer_list.10.weight', 'bert.encoder.fusion_layer_list.10.bias', 'bert.encoder.fusion_layer_list.11.weight', 'bert.encoder.fusion_layer_list.11.bias', 'bert.encoder.word_norm.weight', 'bert.encoder.word_norm.bias', 'bert.encoder.char_norm.weight', 'bert.encoder.char_norm.bias', 'cls.predictions.adv_transform.dense.weight', 'cls.predictions.adv_transform.dense.bias', 'cls.predictions.adv_transform.LayerNorm.weight', 'cls.predictions.adv_transform.LayerNorm.bias', 'cls.predictions.adv_decoder.weight', 'cls.predictions.adv_decoder.bias']\n",
      "02/13/2024 17:15:05 - INFO - modeling.modeling_utils -   Weights from pretrained model not used in CharBertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "02/13/2024 17:15:06 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='Datasets/medical_domain/train_pubmed_full.txt', output_dir='model/output/medical/mlm/base_bert_on_pubmed', eval_data_file='Datasets/medical_domain/val_pubmed_full.txt', char_vocab='CharBERT/data/dict/bert_char_vocab', term_vocab='CharBERT/data/dict/term_vocab', model_type='bert', model_name_or_path='bert-base-uncased', mlm=True, mlm_probability=0.1, adv_probability=0.1, config_name='', data_version='', tokenizer_name='', cache_dir='', block_size=384, do_train=True, do_eval=True, output_debug=False, evaluate_during_training=False, do_lower_case=False, char_maxlen_for_word=6, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=4, gradient_accumulation_steps=1, learning_rate=3e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=10000, input_nraws=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'))\n",
      "/home/temp/anaconda3/envs/flo_nlp/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "02/13/2024 17:15:06 - INFO - __main__ -   ***** Running training *****\n",
      "02/13/2024 17:15:06 - INFO - __main__ -     Num examples = 8168\n",
      "02/13/2024 17:15:06 - INFO - __main__ -     Num Epochs = 3\n",
      "02/13/2024 17:15:06 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
      "02/13/2024 17:15:06 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "02/13/2024 17:15:06 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "02/13/2024 17:15:06 - INFO - __main__ -     Total optimization steps = 6126\n",
      "file_path: Datasets/medical_domain/train_pubmed_full.txt\n",
      "Epoch:   0%|                                              | 0/3 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                       | 0/2042 [00:00<?, ?it/s]\u001b[A02/13/2024 17:15:06 - INFO - __main__ -   Reading the [1]th data block from dataset file at Datasets/medical_domain/train_pubmed_full.txt\n",
      "\n",
      "Iteration:   0%|                             | 1/2042 [00:02<1:40:01,  2.94s/it]\u001b[A\n",
      "Iteration:   0%|                             | 2/2042 [00:03<1:00:30,  1.78s/it]\u001b[A\n",
      "Iteration:   0%|                               | 3/2042 [00:04<47:45,  1.41s/it]\u001b[A\n",
      "Iteration:   0%|                               | 4/2042 [00:05<41:47,  1.23s/it]\u001b[A\n",
      "Iteration:   0%|                               | 5/2042 [00:06<38:33,  1.14s/it]\u001b[A\n",
      "Iteration:   0%|                               | 6/2042 [00:07<36:28,  1.08s/it]\u001b[A\n",
      "Iteration:   0%|                               | 7/2042 [00:08<35:13,  1.04s/it]\u001b[A\n",
      "Iteration:   0%|                               | 8/2042 [00:09<34:24,  1.02s/it]\u001b[A\n",
      "Iteration:   0%|▏                              | 9/2042 [00:10<33:51,  1.00it/s]\u001b[A\n",
      "Iteration:   0%|▏                             | 10/2042 [00:11<33:29,  1.01it/s]\u001b[A\n",
      "Iteration:   1%|▏                             | 11/2042 [00:12<33:16,  1.02it/s]\u001b[A\n",
      "Iteration:   1%|▏                             | 12/2042 [00:13<33:06,  1.02it/s]\u001b[A\n",
      "Iteration:   1%|▏                             | 13/2042 [00:14<33:02,  1.02it/s]\u001b[A\n",
      "Iteration:   1%|▏                             | 14/2042 [00:15<32:58,  1.02it/s]\u001b[A\n",
      "Iteration:   1%|▏                             | 15/2042 [00:16<32:53,  1.03it/s]\u001b[A\n",
      "Iteration:   1%|▏                             | 16/2042 [00:17<32:51,  1.03it/s]\u001b[A\n",
      "Iteration:   1%|▏                             | 17/2042 [00:18<32:49,  1.03it/s]\u001b[A\n",
      "Iteration:   1%|▎                             | 18/2042 [00:19<32:46,  1.03it/s]\u001b[A\n",
      "Iteration:   1%|▎                             | 19/2042 [00:20<32:46,  1.03it/s]\u001b[A\n",
      "Iteration:   1%|▎                             | 20/2042 [00:21<32:39,  1.03it/s]\u001b[A\n",
      "Iteration:   1%|▎                             | 21/2042 [00:22<32:39,  1.03it/s]\u001b[A\n",
      "Iteration:   1%|▎                             | 22/2042 [00:23<32:38,  1.03it/s]\u001b[A\n",
      "Iteration:   1%|▎                             | 23/2042 [00:24<32:39,  1.03it/s]\u001b[A\n",
      "Iteration:   1%|▎                             | 24/2042 [00:25<32:37,  1.03it/s]\u001b[A\n",
      "Iteration:   1%|▎                             | 25/2042 [00:26<32:37,  1.03it/s]\u001b[A\n",
      "Iteration:   1%|▍                             | 26/2042 [00:27<32:36,  1.03it/s]\u001b[A\n",
      "Iteration:   1%|▍                             | 27/2042 [00:28<32:41,  1.03it/s]\u001b[A\n",
      "Iteration:   1%|▍                             | 28/2042 [00:29<32:44,  1.03it/s]\u001b[A\n",
      "Iteration:   1%|▍                             | 29/2042 [00:30<32:44,  1.02it/s]\u001b[A\n",
      "Iteration:   1%|▍                             | 30/2042 [00:31<32:42,  1.03it/s]\u001b[A\n",
      "Iteration:   2%|▍                             | 31/2042 [00:32<32:39,  1.03it/s]\u001b[A\n",
      "Iteration:   2%|▍                             | 32/2042 [00:32<32:37,  1.03it/s]\u001b[A\n",
      "Iteration:   2%|▍                             | 33/2042 [00:33<32:35,  1.03it/s]\u001b[A\n",
      "Iteration:   2%|▍                             | 34/2042 [00:34<32:30,  1.03it/s]\u001b[A\n",
      "Iteration:   2%|▌                             | 35/2042 [00:35<32:27,  1.03it/s]\u001b[A\n",
      "Iteration:   2%|▌                             | 36/2042 [00:36<32:25,  1.03it/s]\u001b[A02/13/2024 17:15:43 - INFO - __main__ -   Reading the [2]th data block from dataset file at Datasets/medical_domain/train_pubmed_full.txt\n",
      "\n",
      "Iteration:   2%|▌                             | 37/2042 [00:37<32:27,  1.03it/s]\u001b[A\n",
      "Iteration:   2%|▌                             | 38/2042 [00:39<34:17,  1.03s/it]\u001b[A\n",
      "Iteration:   2%|▌                             | 39/2042 [00:39<33:42,  1.01s/it]\u001b[A\n",
      "Iteration:   2%|▌                             | 40/2042 [00:40<33:17,  1.00it/s]\u001b[A\n",
      "Iteration:   2%|▌                             | 41/2042 [00:41<32:58,  1.01it/s]\u001b[A\n",
      "Iteration:   2%|▌                             | 42/2042 [00:42<32:41,  1.02it/s]\u001b[A\n",
      "Iteration:   2%|▋                             | 43/2042 [00:43<32:31,  1.02it/s]\u001b[A\n",
      "Iteration:   2%|▋                             | 44/2042 [00:44<32:26,  1.03it/s]\u001b[A\n",
      "Iteration:   2%|▋                             | 45/2042 [00:45<32:19,  1.03it/s]\u001b[A\n",
      "Iteration:   2%|▋                             | 46/2042 [00:46<32:15,  1.03it/s]\u001b[A\n",
      "Iteration:   2%|▋                             | 47/2042 [00:47<32:18,  1.03it/s]\u001b[A\n",
      "Iteration:   2%|▋                             | 48/2042 [00:48<32:26,  1.02it/s]\u001b[A\n",
      "Iteration:   2%|▋                             | 49/2042 [00:49<32:42,  1.02it/s]\u001b[A/home/temp/anaconda3/envs/flo_nlp/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "\n",
      "Iteration:   2%|▋                             | 50/2042 [00:50<32:31,  1.02it/s]\u001b[A\n",
      "Iteration:   2%|▋                             | 51/2042 [00:51<32:14,  1.03it/s]\u001b[A\n",
      "Iteration:   3%|▊                             | 52/2042 [00:52<31:56,  1.04it/s]\u001b[A\n",
      "Iteration:   3%|▊                             | 53/2042 [00:53<31:46,  1.04it/s]\u001b[A\n",
      "Iteration:   3%|▊                             | 54/2042 [00:54<31:43,  1.04it/s]\u001b[A\n",
      "Iteration:   3%|▊                             | 55/2042 [00:55<31:39,  1.05it/s]\u001b[A\n",
      "Iteration:   3%|▊                             | 56/2042 [00:56<31:36,  1.05it/s]\u001b[A\n",
      "Iteration:   3%|▊                             | 57/2042 [00:57<31:37,  1.05it/s]\u001b[A\n",
      "Iteration:   3%|▊                             | 58/2042 [00:58<31:35,  1.05it/s]\u001b[A\n",
      "Iteration:   3%|▊                             | 59/2042 [00:59<31:33,  1.05it/s]\u001b[A\n",
      "Iteration:   3%|▉                             | 60/2042 [01:00<31:32,  1.05it/s]\u001b[A\n",
      "Iteration:   3%|▉                             | 61/2042 [01:01<31:32,  1.05it/s]\u001b[A\n",
      "Iteration:   3%|▉                             | 62/2042 [01:02<31:32,  1.05it/s]\u001b[A\n",
      "Iteration:   3%|▉                             | 63/2042 [01:03<31:32,  1.05it/s]\u001b[A\n",
      "Iteration:   3%|▉                             | 64/2042 [01:04<31:29,  1.05it/s]\u001b[A\n",
      "Iteration:   3%|▉                             | 65/2042 [01:04<31:26,  1.05it/s]\u001b[A\n",
      "Iteration:   3%|▉                             | 66/2042 [01:05<31:29,  1.05it/s]\u001b[A\n",
      "Iteration:   3%|▉                             | 67/2042 [01:06<31:30,  1.04it/s]\u001b[A\n",
      "Iteration:   3%|▉                             | 68/2042 [01:07<31:31,  1.04it/s]\u001b[A\n",
      "Iteration:   3%|█                             | 69/2042 [01:08<31:32,  1.04it/s]\u001b[A\n",
      "Iteration:   3%|█                             | 70/2042 [01:09<31:34,  1.04it/s]\u001b[A\n",
      "Iteration:   3%|█                             | 71/2042 [01:10<31:38,  1.04it/s]\u001b[A\n",
      "Iteration:   4%|█                             | 72/2042 [01:11<32:03,  1.02it/s]\u001b[A\n",
      "Iteration:   4%|█                             | 73/2042 [01:12<32:12,  1.02it/s]\u001b[A\n",
      "Iteration:   4%|█                             | 74/2042 [01:13<32:23,  1.01it/s]\u001b[A\n",
      "Iteration:   4%|█                             | 75/2042 [01:14<32:29,  1.01it/s]\u001b[A02/13/2024 17:16:20 - INFO - __main__ -   Reading the [3]th data block from dataset file at Datasets/medical_domain/train_pubmed_full.txt\n",
      "\n",
      "Iteration:   4%|█                             | 76/2042 [01:15<32:31,  1.01it/s]\u001b[A\n",
      "Iteration:   4%|█▏                            | 77/2042 [01:16<32:22,  1.01it/s]\u001b[A\n",
      "Iteration:   4%|█▏                            | 78/2042 [01:17<32:17,  1.01it/s]\u001b[A\n",
      "Iteration:   4%|█▏                            | 79/2042 [01:18<32:16,  1.01it/s]\u001b[A\n",
      "Iteration:   4%|█▏                            | 80/2042 [01:19<32:13,  1.01it/s]\u001b[A\n",
      "Iteration:   4%|█▏                            | 81/2042 [01:20<32:07,  1.02it/s]\u001b[A\n",
      "Iteration:   4%|█▏                            | 82/2042 [01:21<32:06,  1.02it/s]\u001b[A\n",
      "Iteration:   4%|█▏                            | 83/2042 [01:22<32:02,  1.02it/s]\u001b[A\n",
      "Iteration:   4%|█▏                            | 84/2042 [01:23<31:58,  1.02it/s]\u001b[A\n",
      "Iteration:   4%|█▏                            | 85/2042 [01:24<31:55,  1.02it/s]\u001b[A\n",
      "Iteration:   4%|█▎                            | 86/2042 [01:25<31:53,  1.02it/s]\u001b[A\n",
      "Iteration:   4%|█▎                            | 87/2042 [01:26<31:54,  1.02it/s]\u001b[A\n",
      "Iteration:   4%|█▎                            | 88/2042 [01:27<31:54,  1.02it/s]\u001b[A\n",
      "Iteration:   4%|█▎                            | 89/2042 [01:28<31:53,  1.02it/s]\u001b[A\n",
      "Iteration:   4%|█▎                            | 90/2042 [01:29<31:54,  1.02it/s]\u001b[A\n",
      "Iteration:   4%|█▎                            | 91/2042 [01:30<31:50,  1.02it/s]\u001b[A\n",
      "Iteration:   5%|█▎                            | 92/2042 [01:31<31:51,  1.02it/s]\u001b[A\n",
      "Iteration:   5%|█▎                            | 93/2042 [01:32<31:50,  1.02it/s]\u001b[A\n",
      "Iteration:   5%|█▍                            | 94/2042 [01:33<31:47,  1.02it/s]\u001b[A\n",
      "Iteration:   5%|█▍                            | 95/2042 [01:34<31:50,  1.02it/s]\u001b[A\n",
      "Iteration:   5%|█▍                            | 96/2042 [01:35<31:45,  1.02it/s]\u001b[A\n",
      "Iteration:   5%|█▍                            | 97/2042 [01:36<31:44,  1.02it/s]\u001b[A\n",
      "Iteration:   5%|█▍                            | 98/2042 [01:37<31:43,  1.02it/s]\u001b[A\n",
      "Iteration:   5%|█▍                            | 99/2042 [01:38<31:43,  1.02it/s]\u001b[A\n",
      "Iteration:   5%|█▍                           | 100/2042 [01:39<31:47,  1.02it/s]\u001b[A\n",
      "Iteration:   5%|█▍                           | 101/2042 [01:40<31:50,  1.02it/s]\u001b[A\n",
      "Iteration:   5%|█▍                           | 102/2042 [01:41<31:53,  1.01it/s]\u001b[A\n",
      "Iteration:   5%|█▍                           | 103/2042 [01:42<31:56,  1.01it/s]\u001b[A\n",
      "Iteration:   5%|█▍                           | 104/2042 [01:43<31:54,  1.01it/s]\u001b[A\n",
      "Iteration:   5%|█▍                           | 105/2042 [01:44<31:59,  1.01it/s]\u001b[A\n",
      "Iteration:   5%|█▌                           | 106/2042 [01:45<31:54,  1.01it/s]\u001b[A\n",
      "Iteration:   5%|█▌                           | 107/2042 [01:46<31:54,  1.01it/s]\u001b[A\n",
      "Iteration:   5%|█▌                           | 108/2042 [01:47<31:49,  1.01it/s]\u001b[A\n",
      "Iteration:   5%|█▌                           | 109/2042 [01:48<31:44,  1.01it/s]\u001b[A\n",
      "Iteration:   5%|█▌                           | 110/2042 [01:49<31:43,  1.01it/s]\u001b[A\n",
      "Iteration:   5%|█▌                           | 111/2042 [01:50<31:43,  1.01it/s]\u001b[A\n",
      "Iteration:   5%|█▌                           | 112/2042 [01:51<31:44,  1.01it/s]\u001b[A\n",
      "Iteration:   6%|█▌                           | 113/2042 [01:52<31:51,  1.01it/s]\u001b[A\n",
      "Iteration:   6%|█▌                           | 114/2042 [01:53<31:53,  1.01it/s]\u001b[A02/13/2024 17:16:59 - INFO - __main__ -   Reading the [4]th data block from dataset file at Datasets/medical_domain/train_pubmed_full.txt\n",
      "\n",
      "Iteration:   6%|█▋                           | 115/2042 [01:54<31:52,  1.01it/s]\u001b[A\n",
      "Iteration:   6%|█▋                           | 116/2042 [01:55<35:20,  1.10s/it]\u001b[A\n",
      "Iteration:   6%|█▋                           | 117/2042 [01:56<34:22,  1.07s/it]\u001b[A\n",
      "Iteration:   6%|█▋                           | 118/2042 [01:57<33:40,  1.05s/it]\u001b[A\n",
      "Iteration:   6%|█▋                           | 119/2042 [01:58<33:11,  1.04s/it]\u001b[A\n",
      "Iteration:   6%|█▋                           | 120/2042 [01:59<32:49,  1.02s/it]\u001b[A\n",
      "Iteration:   6%|█▋                           | 121/2042 [02:00<32:34,  1.02s/it]\u001b[A\n",
      "Iteration:   6%|█▋                           | 122/2042 [02:01<32:23,  1.01s/it]\u001b[A\n",
      "Iteration:   6%|█▋                           | 123/2042 [02:02<32:10,  1.01s/it]\u001b[A\n",
      "Iteration:   6%|█▊                           | 124/2042 [02:03<32:05,  1.00s/it]\u001b[A\n",
      "Iteration:   6%|█▊                           | 125/2042 [02:04<32:03,  1.00s/it]\u001b[A\n",
      "Iteration:   6%|█▊                           | 126/2042 [02:05<31:58,  1.00s/it]\u001b[A\n",
      "Iteration:   6%|█▊                           | 127/2042 [02:06<31:56,  1.00s/it]\u001b[A\n",
      "Iteration:   6%|█▊                           | 128/2042 [02:07<31:54,  1.00s/it]\u001b[A\n",
      "Iteration:   6%|█▊                           | 129/2042 [02:08<31:52,  1.00it/s]\u001b[A\n",
      "Iteration:   6%|█▊                           | 130/2042 [02:09<31:47,  1.00it/s]\u001b[A\n",
      "Iteration:   6%|█▊                           | 131/2042 [02:10<31:45,  1.00it/s]\u001b[A\n",
      "Iteration:   6%|█▊                           | 132/2042 [02:11<31:47,  1.00it/s]\u001b[A\n",
      "Iteration:   7%|█▉                           | 133/2042 [02:12<31:46,  1.00it/s]\u001b[A\n",
      "Iteration:   7%|█▉                           | 134/2042 [02:13<31:43,  1.00it/s]\u001b[A\n",
      "Iteration:   7%|█▉                           | 135/2042 [02:14<31:42,  1.00it/s]\u001b[A\n",
      "Iteration:   7%|█▉                           | 136/2042 [02:15<31:41,  1.00it/s]\u001b[A\n",
      "Iteration:   7%|█▉                           | 137/2042 [02:16<31:39,  1.00it/s]\u001b[A\n",
      "Iteration:   7%|█▉                           | 138/2042 [02:17<31:39,  1.00it/s]\u001b[A\n",
      "Iteration:   7%|█▉                           | 139/2042 [02:18<31:38,  1.00it/s]\u001b[A\n",
      "Iteration:   7%|█▉                           | 140/2042 [02:19<31:35,  1.00it/s]\u001b[A\n",
      "Iteration:   7%|██                           | 141/2042 [02:20<31:35,  1.00it/s]\u001b[A\n",
      "Iteration:   7%|██                           | 142/2042 [02:21<31:35,  1.00it/s]\u001b[A\n",
      "Iteration:   7%|██                           | 143/2042 [02:22<31:36,  1.00it/s]\u001b[A\n",
      "Iteration:   7%|██                           | 144/2042 [02:23<31:36,  1.00it/s]\u001b[A\n",
      "Iteration:   7%|██                           | 145/2042 [02:24<31:34,  1.00it/s]\u001b[A\n",
      "Iteration:   7%|██                           | 146/2042 [02:25<31:34,  1.00it/s]\u001b[A\n",
      "Iteration:   7%|██                           | 147/2042 [02:26<31:35,  1.00s/it]\u001b[A\n",
      "Iteration:   7%|██                           | 148/2042 [02:27<31:33,  1.00it/s]\u001b[A\n",
      "Iteration:   7%|██                           | 149/2042 [02:28<31:34,  1.00s/it]\u001b[A\n",
      "Iteration:   7%|██▏                          | 150/2042 [02:29<31:35,  1.00s/it]\u001b[A\n",
      "Iteration:   7%|██▏                          | 151/2042 [02:30<31:35,  1.00s/it]\u001b[A\n",
      "Iteration:   7%|██▏                          | 152/2042 [02:31<31:38,  1.00s/it]\u001b[A02/13/2024 17:17:37 - INFO - __main__ -   Reading the [5]th data block from dataset file at Datasets/medical_domain/train_pubmed_full.txt\n",
      "\n",
      "Iteration:   7%|██▏                          | 153/2042 [02:32<31:33,  1.00s/it]\u001b[A\n",
      "Iteration:   8%|██▏                          | 154/2042 [02:33<32:55,  1.05s/it]\u001b[A\n",
      "Iteration:   8%|██▏                          | 155/2042 [02:34<32:32,  1.03s/it]\u001b[A\n",
      "Iteration:   8%|██▏                          | 156/2042 [02:35<32:18,  1.03s/it]\u001b[A\n",
      "Iteration:   8%|██▏                          | 157/2042 [02:36<32:07,  1.02s/it]\u001b[A\n",
      "Iteration:   8%|██▏                          | 158/2042 [02:37<32:00,  1.02s/it]\u001b[A\n",
      "Iteration:   8%|██▎                          | 159/2042 [02:38<31:54,  1.02s/it]\u001b[A\n",
      "Iteration:   8%|██▎                          | 160/2042 [02:39<31:50,  1.02s/it]\u001b[A\n",
      "Iteration:   8%|██▎                          | 161/2042 [02:40<31:49,  1.02s/it]\u001b[A\n",
      "Iteration:   8%|██▎                          | 162/2042 [02:41<31:51,  1.02s/it]\u001b[A\n",
      "Iteration:   8%|██▎                          | 163/2042 [02:42<31:49,  1.02s/it]\u001b[A\n",
      "Iteration:   8%|██▎                          | 164/2042 [02:43<31:45,  1.01s/it]\u001b[A\n",
      "Iteration:   8%|██▎                          | 165/2042 [02:44<31:42,  1.01s/it]\u001b[A\n",
      "Iteration:   8%|██▎                          | 166/2042 [02:45<31:39,  1.01s/it]\u001b[A\n",
      "Iteration:   8%|██▎                          | 167/2042 [02:46<31:38,  1.01s/it]\u001b[A\n",
      "Iteration:   8%|██▍                          | 168/2042 [02:47<31:38,  1.01s/it]\u001b[A\n",
      "Iteration:   8%|██▍                          | 169/2042 [02:48<31:36,  1.01s/it]\u001b[A\n",
      "Iteration:   8%|██▍                          | 170/2042 [02:49<31:34,  1.01s/it]\u001b[A\n",
      "Iteration:   8%|██▍                          | 171/2042 [02:50<31:35,  1.01s/it]\u001b[A\n",
      "Iteration:   8%|██▍                          | 172/2042 [02:51<31:35,  1.01s/it]\u001b[A\n",
      "Iteration:   8%|██▍                          | 173/2042 [02:52<31:36,  1.01s/it]\u001b[A\n",
      "Iteration:   9%|██▍                          | 174/2042 [02:53<31:37,  1.02s/it]\u001b[A\n",
      "Iteration:   9%|██▍                          | 175/2042 [02:54<31:38,  1.02s/it]\u001b[A\n",
      "Iteration:   9%|██▍                          | 176/2042 [02:55<31:38,  1.02s/it]\u001b[A\n",
      "Iteration:   9%|██▌                          | 177/2042 [02:56<31:38,  1.02s/it]\u001b[A\n",
      "Iteration:   9%|██▌                          | 178/2042 [02:57<31:36,  1.02s/it]\u001b[A\n",
      "Iteration:   9%|██▌                          | 179/2042 [02:58<31:35,  1.02s/it]\u001b[A\n",
      "Iteration:   9%|██▌                          | 180/2042 [02:59<31:30,  1.02s/it]\u001b[A\n",
      "Iteration:   9%|██▌                          | 181/2042 [03:00<31:27,  1.01s/it]\u001b[A\n",
      "Iteration:   9%|██▌                          | 182/2042 [03:01<31:23,  1.01s/it]\u001b[A\n",
      "Iteration:   9%|██▌                          | 183/2042 [03:02<31:20,  1.01s/it]\u001b[A\n",
      "Iteration:   9%|██▌                          | 184/2042 [03:04<31:22,  1.01s/it]\u001b[A\n",
      "Iteration:   9%|██▋                          | 185/2042 [03:05<31:15,  1.01s/it]\u001b[A\n",
      "Iteration:   9%|██▋                          | 186/2042 [03:06<31:13,  1.01s/it]\u001b[A\n",
      "Iteration:   9%|██▋                          | 187/2042 [03:07<31:14,  1.01s/it]\u001b[A\n",
      "Iteration:   9%|██▋                          | 188/2042 [03:08<31:15,  1.01s/it]\u001b[A\n",
      "Iteration:   9%|██▋                          | 189/2042 [03:09<31:13,  1.01s/it]\u001b[A\n",
      "Iteration:   9%|██▋                          | 190/2042 [03:10<31:10,  1.01s/it]\u001b[A\n",
      "Iteration:   9%|██▋                          | 191/2042 [03:11<31:09,  1.01s/it]\u001b[A02/13/2024 17:18:17 - INFO - __main__ -   Reading the [6]th data block from dataset file at Datasets/medical_domain/train_pubmed_full.txt\n",
      "\n",
      "Iteration:   9%|██▋                          | 192/2042 [03:12<31:06,  1.01s/it]\u001b[A\n",
      "Iteration:   9%|██▋                          | 193/2042 [03:13<32:18,  1.05s/it]\u001b[A\n",
      "Iteration:  10%|██▊                          | 194/2042 [03:14<31:50,  1.03s/it]\u001b[A\n",
      "Iteration:  10%|██▊                          | 195/2042 [03:15<31:32,  1.02s/it]\u001b[A\n",
      "Iteration:  10%|██▊                          | 196/2042 [03:16<31:18,  1.02s/it]\u001b[A\n",
      "Iteration:  10%|██▊                          | 197/2042 [03:17<31:10,  1.01s/it]\u001b[A\n",
      "Iteration:  10%|██▊                          | 198/2042 [03:18<31:07,  1.01s/it]\u001b[A\n",
      "Iteration:  10%|██▊                          | 199/2042 [03:19<31:00,  1.01s/it]\u001b[A\n",
      "Iteration:  10%|██▊                          | 200/2042 [03:20<30:55,  1.01s/it]\u001b[A\n",
      "Iteration:  10%|██▊                          | 201/2042 [03:21<30:52,  1.01s/it]\u001b[A\n",
      "Iteration:  10%|██▊                          | 202/2042 [03:22<30:53,  1.01s/it]\u001b[A\n",
      "Iteration:  10%|██▉                          | 203/2042 [03:23<30:50,  1.01s/it]\u001b[A\n",
      "Iteration:  10%|██▉                          | 204/2042 [03:24<30:45,  1.00s/it]\u001b[A\n",
      "Iteration:  10%|██▉                          | 205/2042 [03:25<30:45,  1.00s/it]\u001b[A\n",
      "Iteration:  10%|██▉                          | 206/2042 [03:26<30:46,  1.01s/it]\u001b[A\n",
      "Iteration:  10%|██▉                          | 207/2042 [03:27<30:43,  1.00s/it]\u001b[A\n",
      "Iteration:  10%|██▉                          | 208/2042 [03:28<30:40,  1.00s/it]\u001b[A\n",
      "Iteration:  10%|██▉                          | 209/2042 [03:29<30:39,  1.00s/it]\u001b[A\n",
      "Iteration:  10%|██▉                          | 210/2042 [03:30<30:45,  1.01s/it]\u001b[A\n",
      "Iteration:  10%|██▉                          | 211/2042 [03:31<30:42,  1.01s/it]\u001b[A\n",
      "Iteration:  10%|███                          | 212/2042 [03:32<30:41,  1.01s/it]\u001b[A\n",
      "Iteration:  10%|███                          | 213/2042 [03:33<30:41,  1.01s/it]\u001b[A\n",
      "Iteration:  10%|███                          | 214/2042 [03:34<30:42,  1.01s/it]\u001b[A\n",
      "Iteration:  11%|███                          | 215/2042 [03:35<30:43,  1.01s/it]\u001b[A\n",
      "Iteration:  11%|███                          | 216/2042 [03:36<30:40,  1.01s/it]\u001b[A\n",
      "Iteration:  11%|███                          | 217/2042 [03:37<30:42,  1.01s/it]\u001b[A\n",
      "Iteration:  11%|███                          | 218/2042 [03:38<30:39,  1.01s/it]\u001b[A\n",
      "Iteration:  11%|███                          | 219/2042 [03:39<30:42,  1.01s/it]\u001b[A\n",
      "Iteration:  11%|███                          | 220/2042 [03:40<30:44,  1.01s/it]\u001b[A\n",
      "Iteration:  11%|███▏                         | 221/2042 [03:41<30:39,  1.01s/it]\u001b[A\n",
      "Iteration:  11%|███▏                         | 222/2042 [03:42<30:37,  1.01s/it]\u001b[A\n",
      "Iteration:  11%|███▏                         | 223/2042 [03:43<30:36,  1.01s/it]\u001b[A\n",
      "Iteration:  11%|███▏                         | 224/2042 [03:44<30:35,  1.01s/it]\u001b[A\n",
      "Iteration:  11%|███▏                         | 225/2042 [03:45<30:39,  1.01s/it]\u001b[A\n",
      "Iteration:  11%|███▏                         | 226/2042 [03:46<30:41,  1.01s/it]\u001b[A\n",
      "Iteration:  11%|███▏                         | 227/2042 [03:47<30:45,  1.02s/it]\u001b[A\n",
      "Iteration:  11%|███▏                         | 228/2042 [03:48<30:43,  1.02s/it]\u001b[A\n",
      "Iteration:  11%|███▎                         | 229/2042 [03:49<30:41,  1.02s/it]\u001b[A02/13/2024 17:18:55 - INFO - __main__ -   Reading the [7]th data block from dataset file at Datasets/medical_domain/train_pubmed_full.txt\n",
      "\n",
      "Iteration:  11%|███▎                         | 230/2042 [03:50<30:38,  1.01s/it]\u001b[A\n",
      "Iteration:  11%|███▎                         | 231/2042 [03:51<34:13,  1.13s/it]\u001b[A\n",
      "Iteration:  11%|███▎                         | 232/2042 [03:52<33:07,  1.10s/it]\u001b[A\n",
      "Iteration:  11%|███▎                         | 233/2042 [03:53<32:18,  1.07s/it]\u001b[A\n",
      "Iteration:  11%|███▎                         | 234/2042 [03:54<31:43,  1.05s/it]\u001b[A\n",
      "Iteration:  12%|███▎                         | 235/2042 [03:55<31:15,  1.04s/it]\u001b[A\n",
      "Iteration:  12%|███▎                         | 236/2042 [03:56<30:57,  1.03s/it]\u001b[A\n",
      "Iteration:  12%|███▎                         | 237/2042 [03:57<30:47,  1.02s/it]\u001b[A\n",
      "Iteration:  12%|███▍                         | 238/2042 [03:59<30:43,  1.02s/it]\u001b[A\n",
      "Iteration:  12%|███▍                         | 239/2042 [04:00<30:40,  1.02s/it]\u001b[A\n",
      "Iteration:  12%|███▍                         | 240/2042 [04:01<30:37,  1.02s/it]\u001b[A\n",
      "Iteration:  12%|███▍                         | 241/2042 [04:02<30:34,  1.02s/it]\u001b[A\n",
      "Iteration:  12%|███▍                         | 242/2042 [04:03<30:34,  1.02s/it]\u001b[A\n",
      "Iteration:  12%|███▍                         | 243/2042 [04:04<30:35,  1.02s/it]\u001b[A\n",
      "Iteration:  12%|███▍                         | 244/2042 [04:05<30:37,  1.02s/it]\u001b[A\n",
      "Iteration:  12%|███▍                         | 245/2042 [04:06<30:39,  1.02s/it]\u001b[A\n",
      "Iteration:  12%|███▍                         | 246/2042 [04:07<30:35,  1.02s/it]\u001b[A\n",
      "Iteration:  12%|███▌                         | 247/2042 [04:08<30:38,  1.02s/it]\u001b[A\n",
      "Iteration:  12%|███▌                         | 248/2042 [04:09<30:40,  1.03s/it]\u001b[A\n",
      "Iteration:  12%|███▌                         | 249/2042 [04:10<30:43,  1.03s/it]\u001b[A\n",
      "Iteration:  12%|███▌                         | 250/2042 [04:11<30:46,  1.03s/it]\u001b[A\n",
      "Iteration:  12%|███▌                         | 251/2042 [04:12<30:44,  1.03s/it]\u001b[A\n",
      "Iteration:  12%|███▌                         | 252/2042 [04:13<30:46,  1.03s/it]\u001b[A\n",
      "Iteration:  12%|███▌                         | 253/2042 [04:14<30:43,  1.03s/it]\u001b[A\n",
      "Iteration:  12%|███▌                         | 254/2042 [04:15<30:41,  1.03s/it]\u001b[A\n",
      "Iteration:  12%|███▌                         | 255/2042 [04:16<30:38,  1.03s/it]\u001b[A\n",
      "Iteration:  13%|███▋                         | 256/2042 [04:17<30:38,  1.03s/it]\u001b[A\n",
      "Iteration:  13%|███▋                         | 257/2042 [04:18<30:41,  1.03s/it]\u001b[A\n",
      "Iteration:  13%|███▋                         | 258/2042 [04:19<30:37,  1.03s/it]\u001b[A\n",
      "Iteration:  13%|███▋                         | 259/2042 [04:20<30:36,  1.03s/it]\u001b[A\n",
      "Iteration:  13%|███▋                         | 260/2042 [04:21<30:34,  1.03s/it]\u001b[A\n",
      "Iteration:  13%|███▋                         | 261/2042 [04:22<30:36,  1.03s/it]\u001b[A\n",
      "Iteration:  13%|███▋                         | 262/2042 [04:23<30:35,  1.03s/it]\u001b[A\n",
      "Iteration:  13%|███▋                         | 263/2042 [04:24<30:36,  1.03s/it]\u001b[A\n",
      "Iteration:  13%|███▋                         | 264/2042 [04:25<30:37,  1.03s/it]\u001b[A\n",
      "Iteration:  13%|███▊                         | 265/2042 [04:26<30:36,  1.03s/it]\u001b[A\n",
      "Iteration:  13%|███▊                         | 266/2042 [04:27<30:32,  1.03s/it]\u001b[A\n",
      "Iteration:  13%|███▊                         | 267/2042 [04:28<30:31,  1.03s/it]\u001b[A\n",
      "Iteration:  13%|███▊                         | 268/2042 [04:29<30:25,  1.03s/it]\u001b[A\n",
      "Iteration:  13%|███▊                         | 269/2042 [04:30<30:24,  1.03s/it]\u001b[A02/13/2024 17:19:37 - INFO - __main__ -   Reading the [8]th data block from dataset file at Datasets/medical_domain/train_pubmed_full.txt\n",
      "\n",
      "Iteration:  13%|███▊                         | 270/2042 [04:31<30:22,  1.03s/it]\u001b[A\n",
      "Iteration:  13%|███▊                         | 271/2042 [04:33<31:56,  1.08s/it]\u001b[A\n",
      "Iteration:  13%|███▊                         | 272/2042 [04:34<31:25,  1.07s/it]\u001b[A\n",
      "Iteration:  13%|███▉                         | 273/2042 [04:35<31:06,  1.06s/it]\u001b[A\n",
      "Iteration:  13%|███▉                         | 274/2042 [04:36<30:51,  1.05s/it]\u001b[A\n",
      "Iteration:  13%|███▉                         | 275/2042 [04:37<30:41,  1.04s/it]\u001b[A\n",
      "Iteration:  14%|███▉                         | 276/2042 [04:38<30:34,  1.04s/it]\u001b[A\n",
      "Iteration:  14%|███▉                         | 277/2042 [04:39<30:29,  1.04s/it]\u001b[A\n",
      "Iteration:  14%|███▉                         | 278/2042 [04:40<30:20,  1.03s/it]\u001b[A\n",
      "Iteration:  14%|███▉                         | 279/2042 [04:41<30:06,  1.02s/it]\u001b[A\n",
      "Iteration:  14%|███▉                         | 280/2042 [04:42<29:55,  1.02s/it]\u001b[A\n",
      "Iteration:  14%|███▉                         | 281/2042 [04:43<29:47,  1.02s/it]\u001b[A\n",
      "Iteration:  14%|████                         | 282/2042 [04:44<29:45,  1.01s/it]\u001b[A\n",
      "Iteration:  14%|████                         | 283/2042 [04:45<29:39,  1.01s/it]\u001b[A\n",
      "Iteration:  14%|████                         | 284/2042 [04:46<29:39,  1.01s/it]\u001b[A\n",
      "Iteration:  14%|████                         | 285/2042 [04:47<29:36,  1.01s/it]\u001b[A\n",
      "Iteration:  14%|████                         | 286/2042 [04:48<29:35,  1.01s/it]\u001b[A\n",
      "Iteration:  14%|████                         | 287/2042 [04:49<29:30,  1.01s/it]\u001b[A\n",
      "Iteration:  14%|████                         | 288/2042 [04:50<29:29,  1.01s/it]\u001b[A\n",
      "Iteration:  14%|████                         | 289/2042 [04:51<29:28,  1.01s/it]\u001b[A\n",
      "Iteration:  14%|████                         | 290/2042 [04:52<29:29,  1.01s/it]\u001b[A\n",
      "Iteration:  14%|████▏                        | 291/2042 [04:53<29:27,  1.01s/it]\u001b[A\n",
      "Iteration:  14%|████▏                        | 292/2042 [04:54<29:30,  1.01s/it]\u001b[A\n",
      "Iteration:  14%|████▏                        | 293/2042 [04:55<29:28,  1.01s/it]\u001b[A\n",
      "Iteration:  14%|████▏                        | 294/2042 [04:56<29:27,  1.01s/it]\u001b[A\n",
      "Iteration:  14%|████▏                        | 295/2042 [04:57<29:27,  1.01s/it]\u001b[A\n",
      "Iteration:  14%|████▏                        | 296/2042 [04:58<29:32,  1.02s/it]\u001b[A\n",
      "Iteration:  15%|████▏                        | 297/2042 [04:59<29:35,  1.02s/it]\u001b[A\n",
      "Iteration:  15%|████▏                        | 298/2042 [05:00<29:30,  1.02s/it]\u001b[A\n",
      "Iteration:  15%|████▏                        | 299/2042 [05:01<29:26,  1.01s/it]\u001b[A\n",
      "Iteration:  15%|████▎                        | 300/2042 [05:02<29:22,  1.01s/it]\u001b[A\n",
      "Iteration:  15%|████▎                        | 301/2042 [05:03<29:23,  1.01s/it]\u001b[A\n",
      "Iteration:  15%|████▎                        | 302/2042 [05:04<29:25,  1.01s/it]\u001b[A\n",
      "Iteration:  15%|████▎                        | 303/2042 [05:05<29:25,  1.02s/it]\u001b[A\n",
      "Iteration:  15%|████▎                        | 304/2042 [05:06<29:24,  1.02s/it]\u001b[A\n",
      "Iteration:  15%|████▎                        | 305/2042 [05:07<29:24,  1.02s/it]\u001b[A\n",
      "Iteration:  15%|████▎                        | 306/2042 [05:08<29:19,  1.01s/it]\u001b[A\n",
      "Iteration:  15%|████▎                        | 307/2042 [05:09<29:17,  1.01s/it]\u001b[A\n",
      "Iteration:  15%|████▎                        | 308/2042 [05:10<29:13,  1.01s/it]\u001b[A\n",
      "Iteration:  15%|████▍                        | 309/2042 [05:11<29:12,  1.01s/it]\u001b[A02/13/2024 17:20:17 - INFO - __main__ -   Reading the [9]th data block from dataset file at Datasets/medical_domain/train_pubmed_full.txt\n",
      "\n",
      "Iteration:  15%|████▍                        | 310/2042 [05:12<29:10,  1.01s/it]\u001b[A\n",
      "Iteration:  15%|████▍                        | 311/2042 [05:13<30:18,  1.05s/it]\u001b[A\n",
      "Iteration:  15%|████▍                        | 312/2042 [05:14<29:56,  1.04s/it]\u001b[A\n",
      "Iteration:  15%|████▍                        | 313/2042 [05:15<29:43,  1.03s/it]\u001b[A\n",
      "Iteration:  15%|████▍                        | 314/2042 [05:16<29:37,  1.03s/it]\u001b[A\n",
      "Iteration:  15%|████▍                        | 315/2042 [05:17<29:34,  1.03s/it]\u001b[A\n",
      "Iteration:  15%|████▍                        | 316/2042 [05:18<29:38,  1.03s/it]\u001b[A\n",
      "Iteration:  16%|████▌                        | 317/2042 [05:19<29:35,  1.03s/it]\u001b[A\n",
      "Iteration:  16%|████▌                        | 318/2042 [05:20<29:32,  1.03s/it]\u001b[A\n",
      "Iteration:  16%|████▌                        | 319/2042 [05:22<29:31,  1.03s/it]\u001b[A\n",
      "Iteration:  16%|████▌                        | 320/2042 [05:23<29:23,  1.02s/it]\u001b[A\n",
      "Iteration:  16%|████▌                        | 321/2042 [05:24<29:15,  1.02s/it]\u001b[A\n",
      "Iteration:  16%|████▌                        | 322/2042 [05:25<29:13,  1.02s/it]\u001b[A\n",
      "Iteration:  16%|████▌                        | 323/2042 [05:26<29:09,  1.02s/it]\u001b[A\n",
      "Iteration:  16%|████▌                        | 324/2042 [05:27<29:04,  1.02s/it]\u001b[A\n",
      "Iteration:  16%|████▌                        | 325/2042 [05:28<29:01,  1.01s/it]\u001b[A\n",
      "Iteration:  16%|████▋                        | 326/2042 [05:29<28:57,  1.01s/it]\u001b[A\n",
      "Iteration:  16%|████▋                        | 327/2042 [05:30<28:55,  1.01s/it]\u001b[A\n",
      "Iteration:  16%|████▋                        | 328/2042 [05:31<28:51,  1.01s/it]\u001b[A\n",
      "Iteration:  16%|████▋                        | 329/2042 [05:32<28:48,  1.01s/it]\u001b[A\n",
      "Iteration:  16%|████▋                        | 330/2042 [05:33<28:52,  1.01s/it]\u001b[A\n",
      "Iteration:  16%|████▋                        | 331/2042 [05:34<29:04,  1.02s/it]\u001b[A\n",
      "Iteration:  16%|████▋                        | 332/2042 [05:35<29:06,  1.02s/it]\u001b[A\n",
      "Iteration:  16%|████▋                        | 333/2042 [05:36<29:04,  1.02s/it]\u001b[A\n",
      "Iteration:  16%|████▋                        | 334/2042 [05:37<28:57,  1.02s/it]\u001b[A\n",
      "Iteration:  16%|████▊                        | 335/2042 [05:38<28:51,  1.01s/it]\u001b[A\n",
      "Iteration:  16%|████▊                        | 336/2042 [05:39<28:21,  1.00it/s]\u001b[A\n",
      "Iteration:  17%|████▊                        | 337/2042 [05:40<27:56,  1.02it/s]\u001b[A\n",
      "Iteration:  17%|████▊                        | 338/2042 [05:41<27:37,  1.03it/s]\u001b[A\n",
      "Iteration:  17%|████▊                        | 339/2042 [05:42<27:48,  1.02it/s]\u001b[A\n",
      "Iteration:  17%|████▊                        | 340/2042 [05:43<27:49,  1.02it/s]\u001b[A\n",
      "Iteration:  17%|████▊                        | 341/2042 [05:44<27:41,  1.02it/s]\u001b[A\n",
      "Iteration:  17%|████▊                        | 342/2042 [05:44<27:24,  1.03it/s]\u001b[A\n",
      "Iteration:  17%|████▊                        | 343/2042 [05:46<27:43,  1.02it/s]\u001b[A\n",
      "Iteration:  17%|████▉                        | 344/2042 [05:46<27:31,  1.03it/s]\u001b[A\n",
      "Iteration:  17%|████▉                        | 345/2042 [05:47<27:21,  1.03it/s]\u001b[A\n",
      "Iteration:  17%|████▉                        | 346/2042 [05:48<27:21,  1.03it/s]\u001b[A\n",
      "Iteration:  17%|████▉                        | 347/2042 [05:49<27:34,  1.02it/s]\u001b[A02/13/2024 17:20:56 - INFO - __main__ -   Reading the [10]th data block from dataset file at Datasets/medical_domain/train_pubmed_full.txt\n",
      "\n",
      "Iteration:  17%|████▉                        | 348/2042 [05:50<27:52,  1.01it/s]\u001b[A\n",
      "Iteration:  17%|████▉                        | 349/2042 [05:51<28:12,  1.00it/s]\u001b[A\n",
      "Iteration:  17%|████▉                        | 350/2042 [05:52<28:04,  1.00it/s]\u001b[A\n",
      "Iteration:  17%|████▉                        | 351/2042 [05:53<27:50,  1.01it/s]\u001b[A\n",
      "Iteration:  17%|████▉                        | 352/2042 [05:54<27:35,  1.02it/s]\u001b[A\n",
      "Iteration:  17%|█████                        | 353/2042 [05:55<27:54,  1.01it/s]\u001b[A\n",
      "Iteration:  17%|█████                        | 354/2042 [05:56<28:02,  1.00it/s]\u001b[A\n",
      "Iteration:  17%|█████                        | 355/2042 [05:57<27:58,  1.01it/s]\u001b[A\n",
      "Iteration:  17%|█████                        | 356/2042 [05:58<27:50,  1.01it/s]\u001b[A\n",
      "Iteration:  17%|█████                        | 357/2042 [05:59<27:48,  1.01it/s]\u001b[A\n",
      "Iteration:  18%|█████                        | 358/2042 [06:00<27:39,  1.01it/s]\u001b[A\n",
      "Iteration:  18%|█████                        | 359/2042 [06:01<27:36,  1.02it/s]\u001b[A\n",
      "Iteration:  18%|█████                        | 360/2042 [06:02<27:47,  1.01it/s]\u001b[A\n",
      "Iteration:  18%|█████▏                       | 361/2042 [06:03<27:21,  1.02it/s]\u001b[A\n",
      "Iteration:  18%|█████▏                       | 362/2042 [06:04<27:04,  1.03it/s]\u001b[A\n",
      "Iteration:  18%|█████▏                       | 363/2042 [06:05<26:55,  1.04it/s]\u001b[A\n",
      "Iteration:  18%|█████▏                       | 364/2042 [06:06<26:45,  1.05it/s]\u001b[A\n",
      "Iteration:  18%|█████▏                       | 365/2042 [06:07<27:09,  1.03it/s]\u001b[A\n",
      "Iteration:  18%|█████▏                       | 366/2042 [06:08<27:02,  1.03it/s]\u001b[A\n",
      "Iteration:  18%|█████▏                       | 367/2042 [06:09<26:49,  1.04it/s]\u001b[A\n",
      "Iteration:  18%|█████▏                       | 368/2042 [06:10<26:48,  1.04it/s]\u001b[A\n",
      "Iteration:  18%|█████▏                       | 369/2042 [06:11<26:49,  1.04it/s]\u001b[A\n",
      "Iteration:  18%|█████▎                       | 370/2042 [06:12<26:43,  1.04it/s]\u001b[A\n",
      "Iteration:  18%|█████▎                       | 371/2042 [06:13<26:50,  1.04it/s]\u001b[A\n",
      "Iteration:  18%|█████▎                       | 372/2042 [06:14<26:57,  1.03it/s]\u001b[A\n",
      "Iteration:  18%|█████▎                       | 373/2042 [06:15<26:57,  1.03it/s]\u001b[A\n",
      "Iteration:  18%|█████▎                       | 374/2042 [06:16<26:41,  1.04it/s]\u001b[A\n",
      "Iteration:  18%|█████▎                       | 375/2042 [06:17<26:31,  1.05it/s]\u001b[A\n",
      "Iteration:  18%|█████▎                       | 376/2042 [06:18<26:26,  1.05it/s]\u001b[A\n",
      "Iteration:  18%|█████▎                       | 377/2042 [06:19<26:23,  1.05it/s]\u001b[A\n",
      "Iteration:  19%|█████▎                       | 378/2042 [06:20<26:20,  1.05it/s]\u001b[A\n",
      "Iteration:  19%|█████▍                       | 379/2042 [06:20<26:16,  1.05it/s]\u001b[A\n",
      "Iteration:  19%|█████▍                       | 380/2042 [06:21<26:12,  1.06it/s]\u001b[A\n",
      "Iteration:  19%|█████▍                       | 381/2042 [06:22<26:11,  1.06it/s]\u001b[A\n",
      "Iteration:  19%|█████▍                       | 382/2042 [06:23<26:05,  1.06it/s]\u001b[A\n",
      "Iteration:  19%|█████▍                       | 383/2042 [06:24<26:05,  1.06it/s]\u001b[A\n",
      "Iteration:  19%|█████▍                       | 384/2042 [06:25<26:01,  1.06it/s]\u001b[A\n",
      "Iteration:  19%|█████▍                       | 385/2042 [06:26<26:01,  1.06it/s]\u001b[A\n",
      "Iteration:  19%|█████▍                       | 386/2042 [06:27<25:58,  1.06it/s]\u001b[A02/13/2024 17:21:33 - INFO - __main__ -   Reading the [11]th data block from dataset file at Datasets/medical_domain/train_pubmed_full.txt\n",
      "\n",
      "Iteration:  19%|█████▍                       | 387/2042 [06:28<26:00,  1.06it/s]\u001b[A\n",
      "Iteration:  19%|█████▌                       | 388/2042 [06:29<28:02,  1.02s/it]\u001b[A\n",
      "Iteration:  19%|█████▌                       | 389/2042 [06:30<27:31,  1.00it/s]\u001b[A\n",
      "Iteration:  19%|█████▌                       | 390/2042 [06:31<27:09,  1.01it/s]\u001b[A\n",
      "Iteration:  19%|█████▌                       | 391/2042 [06:32<26:57,  1.02it/s]\u001b[A\n",
      "Iteration:  19%|█████▌                       | 392/2042 [06:33<26:41,  1.03it/s]\u001b[A\n",
      "Iteration:  19%|█████▌                       | 393/2042 [06:34<26:38,  1.03it/s]\u001b[A\n",
      "Iteration:  19%|█████▌                       | 394/2042 [06:35<26:30,  1.04it/s]\u001b[A\n",
      "Iteration:  19%|█████▌                       | 395/2042 [06:36<26:25,  1.04it/s]\u001b[A\n",
      "Iteration:  19%|█████▌                       | 396/2042 [06:37<26:23,  1.04it/s]\u001b[A\n",
      "Iteration:  19%|█████▋                       | 397/2042 [06:38<26:14,  1.04it/s]\u001b[A\n",
      "Iteration:  19%|█████▋                       | 398/2042 [06:39<26:13,  1.05it/s]\u001b[A\n",
      "Iteration:  20%|█████▋                       | 399/2042 [06:40<26:07,  1.05it/s]\u001b[A\n",
      "Iteration:  20%|█████▋                       | 400/2042 [06:41<26:04,  1.05it/s]\u001b[A\n",
      "Iteration:  20%|█████▋                       | 401/2042 [06:42<26:00,  1.05it/s]\u001b[A\n",
      "Iteration:  20%|█████▋                       | 402/2042 [06:43<25:56,  1.05it/s]\u001b[A\n",
      "Iteration:  20%|█████▋                       | 403/2042 [06:43<25:49,  1.06it/s]\u001b[A\n",
      "Iteration:  20%|█████▋                       | 404/2042 [06:44<25:54,  1.05it/s]\u001b[A\n",
      "Iteration:  20%|█████▊                       | 405/2042 [06:45<25:54,  1.05it/s]\u001b[A\n",
      "Iteration:  20%|█████▊                       | 406/2042 [06:46<25:49,  1.06it/s]\u001b[A\n",
      "Iteration:  20%|█████▊                       | 407/2042 [06:47<25:47,  1.06it/s]\u001b[A\n",
      "Iteration:  20%|█████▊                       | 408/2042 [06:48<25:50,  1.05it/s]\u001b[A\n",
      "Iteration:  20%|█████▊                       | 409/2042 [06:49<25:48,  1.05it/s]\u001b[A\n",
      "Iteration:  20%|█████▊                       | 410/2042 [06:50<25:47,  1.05it/s]\u001b[A\n",
      "Iteration:  20%|█████▊                       | 411/2042 [06:51<25:42,  1.06it/s]\u001b[A\n",
      "Iteration:  20%|█████▊                       | 412/2042 [06:52<25:41,  1.06it/s]\u001b[A\n",
      "Iteration:  20%|█████▊                       | 413/2042 [06:53<25:48,  1.05it/s]\u001b[A\n",
      "Iteration:  20%|█████▉                       | 414/2042 [06:54<25:46,  1.05it/s]\u001b[A\n",
      "Iteration:  20%|█████▉                       | 415/2042 [06:55<25:42,  1.05it/s]\u001b[A\n",
      "Iteration:  20%|█████▉                       | 416/2042 [06:56<25:45,  1.05it/s]\u001b[A\n",
      "Iteration:  20%|█████▉                       | 417/2042 [06:57<25:45,  1.05it/s]\u001b[A\n",
      "Iteration:  20%|█████▉                       | 418/2042 [06:58<25:43,  1.05it/s]\u001b[A\n",
      "Iteration:  21%|█████▉                       | 419/2042 [06:59<25:36,  1.06it/s]\u001b[A\n",
      "Iteration:  21%|█████▉                       | 420/2042 [07:00<25:33,  1.06it/s]\u001b[A\n",
      "Iteration:  21%|█████▉                       | 421/2042 [07:01<25:29,  1.06it/s]\u001b[A\n",
      "Iteration:  21%|█████▉                       | 422/2042 [07:01<25:28,  1.06it/s]\u001b[A\n",
      "Iteration:  21%|██████                       | 423/2042 [07:02<25:34,  1.05it/s]\u001b[A\n",
      "Iteration:  21%|██████                       | 424/2042 [07:03<25:33,  1.06it/s]\u001b[A\n",
      "Iteration:  21%|██████                       | 425/2042 [07:04<25:31,  1.06it/s]\u001b[A02/13/2024 17:22:11 - INFO - __main__ -   Reading the [12]th data block from dataset file at Datasets/medical_domain/train_pubmed_full.txt\n",
      "\n",
      "Iteration:  21%|██████                       | 426/2042 [07:05<25:29,  1.06it/s]\u001b[A\n",
      "Iteration:  21%|██████                       | 427/2042 [07:06<26:05,  1.03it/s]\u001b[A\n",
      "Iteration:  21%|██████                       | 428/2042 [07:07<25:52,  1.04it/s]\u001b[A\n",
      "Iteration:  21%|██████                       | 429/2042 [07:08<26:13,  1.02it/s]\u001b[A\n",
      "Iteration:  21%|██████                       | 430/2042 [07:09<26:20,  1.02it/s]\u001b[A\n",
      "Iteration:  21%|██████                       | 431/2042 [07:10<26:22,  1.02it/s]\u001b[A\n",
      "Iteration:  21%|██████▏                      | 432/2042 [07:11<26:22,  1.02it/s]\u001b[A\n",
      "Iteration:  21%|██████▏                      | 433/2042 [07:12<26:26,  1.01it/s]\u001b[A\n",
      "Iteration:  21%|██████▏                      | 434/2042 [07:13<26:04,  1.03it/s]\u001b[A\n",
      "Iteration:  21%|██████▏                      | 435/2042 [07:14<25:49,  1.04it/s]\u001b[A\n",
      "Iteration:  21%|██████▏                      | 436/2042 [07:15<26:07,  1.02it/s]\u001b[A\n",
      "Iteration:  21%|██████▏                      | 437/2042 [07:16<26:19,  1.02it/s]\u001b[A\n",
      "Iteration:  21%|██████▏                      | 438/2042 [07:17<26:38,  1.00it/s]\u001b[A\n",
      "Iteration:  21%|██████▏                      | 439/2042 [07:18<26:12,  1.02it/s]\u001b[A\n",
      "Iteration:  22%|██████▏                      | 440/2042 [07:19<25:52,  1.03it/s]\u001b[A\n",
      "Iteration:  22%|██████▎                      | 441/2042 [07:20<25:56,  1.03it/s]\u001b[A\n",
      "Iteration:  22%|██████▎                      | 442/2042 [07:21<25:54,  1.03it/s]\u001b[A\n",
      "Iteration:  22%|██████▎                      | 443/2042 [07:22<25:45,  1.03it/s]\u001b[A\n",
      "Iteration:  22%|██████▎                      | 444/2042 [07:23<25:37,  1.04it/s]\u001b[A\n",
      "Iteration:  22%|██████▎                      | 445/2042 [07:24<25:27,  1.05it/s]\u001b[A\n",
      "Iteration:  22%|██████▎                      | 446/2042 [07:25<25:19,  1.05it/s]\u001b[A\n",
      "Iteration:  22%|██████▎                      | 447/2042 [07:26<25:30,  1.04it/s]\u001b[A\n",
      "Iteration:  22%|██████▎                      | 448/2042 [07:27<25:18,  1.05it/s]\u001b[A\n",
      "Iteration:  22%|██████▍                      | 449/2042 [07:28<25:10,  1.05it/s]\u001b[A\n",
      "Iteration:  22%|██████▍                      | 450/2042 [07:29<25:06,  1.06it/s]\u001b[A\n",
      "Iteration:  22%|██████▍                      | 451/2042 [07:29<25:00,  1.06it/s]\u001b[A\n",
      "Iteration:  22%|██████▍                      | 452/2042 [07:30<24:59,  1.06it/s]\u001b[A\n",
      "Iteration:  22%|██████▍                      | 453/2042 [07:31<24:55,  1.06it/s]\u001b[A\n",
      "Iteration:  22%|██████▍                      | 454/2042 [07:32<24:55,  1.06it/s]\u001b[A\n",
      "Iteration:  22%|██████▍                      | 455/2042 [07:33<24:54,  1.06it/s]\u001b[A\n",
      "Iteration:  22%|██████▍                      | 456/2042 [07:34<24:50,  1.06it/s]\u001b[A\n",
      "Iteration:  22%|██████▍                      | 457/2042 [07:35<24:49,  1.06it/s]\u001b[A\n",
      "Iteration:  22%|██████▌                      | 458/2042 [07:36<24:49,  1.06it/s]\u001b[A\n",
      "Iteration:  22%|██████▌                      | 459/2042 [07:37<24:48,  1.06it/s]\u001b[A\n",
      "Iteration:  23%|██████▌                      | 460/2042 [07:38<24:47,  1.06it/s]\u001b[A\n",
      "Iteration:  23%|██████▌                      | 461/2042 [07:39<24:43,  1.07it/s]\u001b[A\n",
      "Iteration:  23%|██████▌                      | 462/2042 [07:40<24:45,  1.06it/s]\u001b[A\n",
      "Iteration:  23%|██████▌                      | 463/2042 [07:41<24:44,  1.06it/s]\u001b[A\n",
      "Iteration:  23%|██████▌                      | 464/2042 [07:42<24:57,  1.05it/s]\u001b[A02/13/2024 17:22:48 - INFO - __main__ -   Reading the [13]th data block from dataset file at Datasets/medical_domain/train_pubmed_full.txt\n",
      "\n",
      "Iteration:  23%|██████▌                      | 465/2042 [07:43<25:10,  1.04it/s]\u001b[A\n",
      "Iteration:  23%|██████▌                      | 466/2042 [07:44<25:27,  1.03it/s]\u001b[A\n",
      "Iteration:  23%|██████▋                      | 467/2042 [07:45<25:10,  1.04it/s]\u001b[A\n",
      "Iteration:  23%|██████▋                      | 468/2042 [07:46<25:00,  1.05it/s]\u001b[A\n",
      "Iteration:  23%|██████▋                      | 469/2042 [07:47<24:58,  1.05it/s]\u001b[A\n",
      "Iteration:  23%|██████▋                      | 470/2042 [07:47<24:59,  1.05it/s]\u001b[A\n",
      "Iteration:  23%|██████▋                      | 471/2042 [07:48<24:55,  1.05it/s]\u001b[A\n",
      "Iteration:  23%|██████▋                      | 472/2042 [07:49<24:51,  1.05it/s]\u001b[A\n",
      "Iteration:  23%|██████▋                      | 473/2042 [07:50<24:51,  1.05it/s]\u001b[A\n",
      "Iteration:  23%|██████▋                      | 474/2042 [07:51<24:47,  1.05it/s]\u001b[A\n",
      "Iteration:  23%|██████▋                      | 475/2042 [07:52<24:43,  1.06it/s]\u001b[A\n",
      "Iteration:  23%|██████▊                      | 476/2042 [07:53<24:39,  1.06it/s]\u001b[A\n",
      "Iteration:  23%|██████▊                      | 477/2042 [07:54<24:36,  1.06it/s]\u001b[A\n",
      "Iteration:  23%|██████▊                      | 478/2042 [07:55<24:42,  1.06it/s]\u001b[A\n",
      "Iteration:  23%|██████▊                      | 479/2042 [07:56<24:52,  1.05it/s]\u001b[A\n",
      "Iteration:  24%|██████▊                      | 480/2042 [07:57<24:49,  1.05it/s]\u001b[A\n",
      "Iteration:  24%|██████▊                      | 481/2042 [07:58<24:50,  1.05it/s]\u001b[A\n",
      "Iteration:  24%|██████▊                      | 482/2042 [07:59<24:52,  1.04it/s]\u001b[A\n",
      "Iteration:  24%|██████▊                      | 483/2042 [08:00<25:00,  1.04it/s]\u001b[A\n",
      "Iteration:  24%|██████▊                      | 484/2042 [08:01<24:50,  1.05it/s]\u001b[A\n",
      "Iteration:  24%|██████▉                      | 485/2042 [08:02<24:44,  1.05it/s]\u001b[A\n",
      "Iteration:  24%|██████▉                      | 486/2042 [08:03<24:49,  1.04it/s]\u001b[A\n",
      "Iteration:  24%|██████▉                      | 487/2042 [08:04<25:00,  1.04it/s]\u001b[A\n",
      "Iteration:  24%|██████▉                      | 488/2042 [08:05<25:24,  1.02it/s]\u001b[A\n",
      "Iteration:  24%|██████▉                      | 489/2042 [08:06<25:44,  1.01it/s]\u001b[A\n",
      "Iteration:  24%|██████▉                      | 490/2042 [08:07<25:53,  1.00s/it]\u001b[A\n",
      "Iteration:  24%|██████▉                      | 491/2042 [08:08<26:00,  1.01s/it]\u001b[A\n",
      "Iteration:  24%|██████▉                      | 492/2042 [08:09<26:06,  1.01s/it]\u001b[A\n",
      "Iteration:  24%|███████                      | 493/2042 [08:10<26:09,  1.01s/it]\u001b[A\n",
      "Iteration:  24%|███████                      | 494/2042 [08:11<26:10,  1.01s/it]\u001b[A\n",
      "Iteration:  24%|███████                      | 495/2042 [08:12<26:11,  1.02s/it]\u001b[A\n",
      "Iteration:  24%|███████                      | 496/2042 [08:13<26:13,  1.02s/it]\u001b[A\n",
      "Iteration:  24%|███████                      | 497/2042 [08:14<26:14,  1.02s/it]\u001b[A\n",
      "Iteration:  24%|███████                      | 498/2042 [08:15<26:14,  1.02s/it]\u001b[A\n",
      "Iteration:  24%|███████                      | 499/2042 [08:16<26:15,  1.02s/it]\u001b[A\n",
      "Iteration:  24%|███████                      | 500/2042 [08:17<26:17,  1.02s/it]\u001b[A\n",
      "Iteration:  25%|███████                      | 501/2042 [08:18<26:16,  1.02s/it]\u001b[A\n",
      "Iteration:  25%|███████▏                     | 502/2042 [08:19<26:11,  1.02s/it]\u001b[A02/13/2024 17:23:25 - INFO - __main__ -   Reading the [14]th data block from dataset file at Datasets/medical_domain/train_pubmed_full.txt\n",
      "\n",
      "Iteration:  25%|███████▏                     | 503/2042 [08:20<26:07,  1.02s/it]\u001b[A\n",
      "Iteration:  25%|███████▏                     | 504/2042 [08:21<29:01,  1.13s/it]\u001b[A\n",
      "Iteration:  25%|███████▏                     | 505/2042 [08:22<28:05,  1.10s/it]\u001b[A\n",
      "Iteration:  25%|███████▏                     | 506/2042 [08:23<27:26,  1.07s/it]\u001b[A\n",
      "Iteration:  25%|███████▏                     | 507/2042 [08:24<26:59,  1.06s/it]\u001b[A\n",
      "Iteration:  25%|███████▏                     | 508/2042 [08:25<26:40,  1.04s/it]\u001b[A\n",
      "Iteration:  25%|███████▏                     | 509/2042 [08:27<26:25,  1.03s/it]\u001b[A\n",
      "Iteration:  25%|███████▏                     | 510/2042 [08:28<26:14,  1.03s/it]\u001b[A\n",
      "Iteration:  25%|███████▎                     | 511/2042 [08:29<26:07,  1.02s/it]\u001b[A\n",
      "Iteration:  25%|███████▎                     | 512/2042 [08:30<26:03,  1.02s/it]\u001b[A\n",
      "Iteration:  25%|███████▎                     | 513/2042 [08:31<27:48,  1.09s/it]\u001b[A\n",
      "Iteration:  25%|███████▎                     | 514/2042 [08:32<27:13,  1.07s/it]\u001b[A\n",
      "Iteration:  25%|███████▎                     | 515/2042 [08:33<26:51,  1.06s/it]\u001b[A\n",
      "Iteration:  25%|███████▎                     | 516/2042 [08:34<26:33,  1.04s/it]\u001b[A^C\n"
     ]
    }
   ],
   "source": [
    "!python CharBERT/run_lm_finetuning.py \\\n",
    "    --model_type bert \\\n",
    "    --model_name_or_path bert-base-uncased \\\n",
    "    --output_dir model/output/medical/mlm/base_bert_on_pubmed \\\n",
    "    --train_data_file datasets/medical_domain/mlm/train_pubmed_full.txt \\\n",
    "    --eval_data_file  datasets/medical_domain/mlm/val_pubmed_full.txt \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --term_vocab CharBERT/data/dict/term_vocab \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --char_vocab CharBERT/data/dict/bert_char_vocab \\\n",
    "    --mlm_probability 0.10 \\\n",
    "    --input_nraws 1000 \\\n",
    "    --per_gpu_train_batch_size 4 \\\n",
    "    --per_gpu_eval_batch_size 4 \\\n",
    "    --save_steps 10000 \\\n",
    "    --block_size 384 \\\n",
    "    --mlm \\\n",
    "    --overwrite_output_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain - RUN MLM \n",
    "\n",
    "BertCheckpoint: BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\n",
    "\n",
    "Dataset: PubMed\n",
    "\n",
    "Output Dir: model/output/medical/mlm/biomed_bert_on_pubmed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash model/download_biomednlp_bert.bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-13 15:41:55.595437: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-13 15:41:55.598000: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-13 15:41:55.627460: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-13 15:41:55.627493: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-13 15:41:55.628337: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-13 15:41:55.633508: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-13 15:41:56.415038: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "02/13/2024 15:41:57 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "02/13/2024 15:41:57 - INFO - modeling.configuration_utils -   loading configuration file model/input/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext/config.json\n",
      "02/13/2024 15:41:57 - INFO - modeling.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "02/13/2024 15:41:57 - INFO - modeling.modeling_utils -   loading weights file model/input/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext/pytorch_model.bin\n",
      "02/13/2024 15:41:59 - INFO - modeling.modeling_utils -   Weights of CharBertForMaskedLM not initialized from pretrained model: ['bert.char_embeddings.char_embeddings.weight', 'bert.char_embeddings.rnn_layer.weight_ih_l0', 'bert.char_embeddings.rnn_layer.weight_hh_l0', 'bert.char_embeddings.rnn_layer.bias_ih_l0', 'bert.char_embeddings.rnn_layer.bias_hh_l0', 'bert.char_embeddings.rnn_layer.weight_ih_l0_reverse', 'bert.char_embeddings.rnn_layer.weight_hh_l0_reverse', 'bert.char_embeddings.rnn_layer.bias_ih_l0_reverse', 'bert.char_embeddings.rnn_layer.bias_hh_l0_reverse', 'bert.encoder.word_linear1.weight', 'bert.encoder.char_linear1.weight', 'bert.encoder.fusion_layer_list.0.weight', 'bert.encoder.fusion_layer_list.0.bias', 'bert.encoder.fusion_layer_list.1.weight', 'bert.encoder.fusion_layer_list.1.bias', 'bert.encoder.fusion_layer_list.2.weight', 'bert.encoder.fusion_layer_list.2.bias', 'bert.encoder.fusion_layer_list.3.weight', 'bert.encoder.fusion_layer_list.3.bias', 'bert.encoder.fusion_layer_list.4.weight', 'bert.encoder.fusion_layer_list.4.bias', 'bert.encoder.fusion_layer_list.5.weight', 'bert.encoder.fusion_layer_list.5.bias', 'bert.encoder.fusion_layer_list.6.weight', 'bert.encoder.fusion_layer_list.6.bias', 'bert.encoder.fusion_layer_list.7.weight', 'bert.encoder.fusion_layer_list.7.bias', 'bert.encoder.fusion_layer_list.8.weight', 'bert.encoder.fusion_layer_list.8.bias', 'bert.encoder.fusion_layer_list.9.weight', 'bert.encoder.fusion_layer_list.9.bias', 'bert.encoder.fusion_layer_list.10.weight', 'bert.encoder.fusion_layer_list.10.bias', 'bert.encoder.fusion_layer_list.11.weight', 'bert.encoder.fusion_layer_list.11.bias', 'bert.encoder.word_norm.weight', 'bert.encoder.word_norm.bias', 'bert.encoder.char_norm.weight', 'bert.encoder.char_norm.bias', 'cls.predictions.adv_transform.dense.weight', 'cls.predictions.adv_transform.dense.bias', 'cls.predictions.adv_transform.LayerNorm.weight', 'cls.predictions.adv_transform.LayerNorm.bias', 'cls.predictions.adv_decoder.weight', 'cls.predictions.adv_decoder.bias']\n",
      "02/13/2024 15:41:59 - INFO - modeling.modeling_utils -   Weights from pretrained model not used in CharBertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias']\n",
      "02/13/2024 15:42:00 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='datasets/multilingual/wiki_eng_train.csv', output_dir='model/output/mlm/wikil_eng_train', eval_data_file='datasets/multilingual/wiki_eng_test.csv', char_vocab='CharBERT/data/dict/bert_char_vocab', term_vocab='CharBERT/data/dict/term_vocab', model_type='bert', model_name_or_path='model/input/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext', mlm=True, mlm_probability=0.1, adv_probability=0.1, config_name='', data_version='', tokenizer_name='', cache_dir='', block_size=384, do_train=True, do_eval=True, output_debug=False, evaluate_during_training=False, do_lower_case=False, char_maxlen_for_word=6, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=4, gradient_accumulation_steps=1, learning_rate=3e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=10000, input_nraws=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'))\n",
      "/home/temp/anaconda3/envs/flo_nlp/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "02/13/2024 15:42:00 - INFO - __main__ -   ***** Running training *****\n",
      "02/13/2024 15:42:00 - INFO - __main__ -     Num examples = 415670\n",
      "02/13/2024 15:42:00 - INFO - __main__ -     Num Epochs = 10\n",
      "02/13/2024 15:42:00 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
      "02/13/2024 15:42:00 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "02/13/2024 15:42:00 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "02/13/2024 15:42:00 - INFO - __main__ -     Total optimization steps = 1039180\n",
      "02/13/2024 15:42:00 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step\n",
      "02/13/2024 15:42:00 - INFO - __main__ -     Continuing training from epoch 0\n",
      "02/13/2024 15:42:00 - INFO - __main__ -     Continuing training from global step 0\n",
      "02/13/2024 15:42:00 - INFO - __main__ -     Will skip the first 0 steps in the first epoch\n",
      "QUI\n",
      "device DEVICE DEVICE DEVICE DEVICE: cuda\n",
      "cls: <class 'modeling.configuration_bert.BertConfig'>\n",
      "pretrained_model_name_or_path: model/input/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\n",
      "pretrained_model_name_or_path: model/input/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\n",
      "cls.pretrained_config_archive_map: {'bert-base-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json', 'bert-large-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json', 'bert-base-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json', 'bert-large-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-config.json', 'bert-base-multilingual-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json', 'bert-base-multilingual-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json', 'bert-base-chinese': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json', 'bert-base-german-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-config.json', 'bert-large-uncased-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-config.json', 'bert-large-cased-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-config.json', 'bert-large-uncased-whole-word-masking-finetuned-squad': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-config.json', 'bert-large-cased-whole-word-masking-finetuned-squad': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-config.json', 'bert-base-cased-finetuned-mrpc': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-config.json', 'bert-base-german-dbmdz-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json', 'bert-base-german-dbmdz-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-uncased-config.json', 'bert-base-japanese': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-config.json', 'bert-base-japanese-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking-config.json', 'bert-base-japanese-char': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-char-config.json', 'bert-base-japanese-char-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-char-whole-word-masking-config.json', 'bert-base-finnish-cased-v1': 'https://s3.amazonaws.com/models.huggingface.co/bert/TurkuNLP/bert-base-finnish-cased-v1/config.json', 'bert-base-finnish-uncased-v1': 'https://s3.amazonaws.com/models.huggingface.co/bert/TurkuNLP/bert-base-finnish-uncased-v1/config.json'}\n",
      "file_path: datasets/multilingual/wiki_eng_train.csv\n",
      "Epoch:   0%|                                             | 0/10 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                     | 0/103918 [00:00<?, ?it/s]\u001b[A02/13/2024 15:42:00 - INFO - __main__ -   Reading the [1]th data block from dataset file at datasets/multilingual/wiki_eng_train.csv\n",
      "\n",
      "Iteration:   0%|                          | 1/103918 [00:01<53:36:49,  1.86s/it]\u001b[A\n",
      "Iteration:   0%|                          | 2/103918 [00:02<38:50:16,  1.35s/it]\u001b[A\n",
      "Iteration:   0%|                          | 3/103918 [00:03<34:18:22,  1.19s/it]\u001b[A\n",
      "Iteration:   0%|                          | 4/103918 [00:04<32:09:48,  1.11s/it]\u001b[A\n",
      "Iteration:   0%|                          | 5/103918 [00:05<31:01:08,  1.07s/it]\u001b[A\n",
      "Iteration:   0%|                          | 6/103918 [00:06<30:17:50,  1.05s/it]\u001b[A\n",
      "Iteration:   0%|                          | 7/103918 [00:07<29:49:35,  1.03s/it]\u001b[A\n",
      "Iteration:   0%|                          | 8/103918 [00:08<29:31:28,  1.02s/it]\u001b[A\n",
      "Iteration:   0%|                          | 9/103918 [00:09<29:07:15,  1.01s/it]\u001b[A\n",
      "Iteration:   0%|                         | 10/103918 [00:10<28:48:52,  1.00it/s]\u001b[A02/13/2024 15:42:11 - INFO - __main__ -   Reading the [2]th data block from dataset file at datasets/multilingual/wiki_eng_train.csv\n",
      "\n",
      "Iteration:   0%|                         | 11/103918 [00:11<28:47:14,  1.00it/s]\u001b[A\n",
      "Iteration:   0%|                         | 12/103918 [00:12<28:29:53,  1.01it/s]\u001b[A\n",
      "Iteration:   0%|                         | 13/103918 [00:13<28:18:21,  1.02it/s]\u001b[A\n",
      "Iteration:   0%|                         | 14/103918 [00:14<28:19:47,  1.02it/s]\u001b[A\n",
      "Iteration:   0%|                         | 15/103918 [00:15<28:16:36,  1.02it/s]\u001b[A\n",
      "Iteration:   0%|                         | 16/103918 [00:16<28:09:41,  1.02it/s]\u001b[A\n",
      "Iteration:   0%|                         | 17/103918 [00:17<28:14:32,  1.02it/s]\u001b[A\n",
      "Iteration:   0%|                         | 18/103918 [00:18<28:10:34,  1.02it/s]\u001b[A\n",
      "Iteration:   0%|                         | 19/103918 [00:19<28:13:44,  1.02it/s]\u001b[A\n",
      "Iteration:   0%|                         | 20/103918 [00:20<28:16:12,  1.02it/s]\u001b[A\n",
      "Iteration:   0%|                         | 21/103918 [00:21<28:08:45,  1.03it/s]\u001b[A\n",
      "Iteration:   0%|                         | 22/103918 [00:22<28:07:18,  1.03it/s]\u001b[A\n",
      "Iteration:   0%|                         | 23/103918 [00:23<28:04:15,  1.03it/s]\u001b[A\n",
      "Iteration:   0%|                         | 24/103918 [00:24<28:03:56,  1.03it/s]\u001b[A\n",
      "Iteration:   0%|                         | 25/103918 [00:25<28:01:31,  1.03it/s]\u001b[A02/13/2024 15:42:26 - INFO - __main__ -   Reading the [3]th data block from dataset file at datasets/multilingual/wiki_eng_train.csv\n",
      "\n",
      "Iteration:   0%|                         | 26/103918 [00:26<28:00:00,  1.03it/s]\u001b[A\n",
      "Iteration:   0%|                         | 27/103918 [00:27<27:57:53,  1.03it/s]\u001b[A\n",
      "Iteration:   0%|                         | 28/103918 [00:28<27:59:55,  1.03it/s]\u001b[A\n",
      "Iteration:   0%|                         | 29/103918 [00:29<27:57:58,  1.03it/s]\u001b[A\n",
      "Iteration:   0%|                         | 30/103918 [00:30<27:59:08,  1.03it/s]\u001b[A\n",
      "Iteration:   0%|                         | 31/103918 [00:31<27:59:38,  1.03it/s]\u001b[A\n",
      "Iteration:   0%|                         | 32/103918 [00:32<28:00:33,  1.03it/s]\u001b[A\n",
      "Iteration:   0%|                         | 33/103918 [00:33<28:00:00,  1.03it/s]\u001b[A\n",
      "Iteration:   0%|                         | 34/103918 [00:34<28:07:41,  1.03it/s]\u001b[A\n",
      "Iteration:   0%|                         | 35/103918 [00:35<28:07:55,  1.03it/s]\u001b[A\n",
      "Iteration:   0%|                         | 36/103918 [00:36<28:24:10,  1.02it/s]\u001b[A\n",
      "Iteration:   0%|                         | 37/103918 [00:37<28:29:53,  1.01it/s]\u001b[A02/13/2024 15:42:37 - INFO - __main__ -   Reading the [4]th data block from dataset file at datasets/multilingual/wiki_eng_train.csv\n",
      "\n",
      "Iteration:   0%|                         | 38/103918 [00:38<28:33:39,  1.01it/s]\u001b[A\n",
      "Iteration:   0%|                         | 39/103918 [00:39<28:40:35,  1.01it/s]\u001b[A\n",
      "Iteration:   0%|                         | 40/103918 [00:40<28:53:12,  1.00s/it]\u001b[A\n",
      "Iteration:   0%|                         | 41/103918 [00:41<28:58:19,  1.00s/it]\u001b[A\n",
      "Iteration:   0%|                         | 42/103918 [00:42<29:01:17,  1.01s/it]\u001b[A^C\n"
     ]
    }
   ],
   "source": [
    "!python CharBERT/run_lm_finetuning.py \\\n",
    "    --model_type bert \\\n",
    "    --model_name_or_path model/input/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext \\\n",
    "    --output_dir model/output/medical/mlm/biomed_bert_on_pubmed \\\n",
    "    --train_data_file datasets/medical_domain/mlm/train_pubmed_full.txt  \\\n",
    "    --eval_data_file datasets/medical_domain/mlm/val_pubmed_full.txt  \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --term_vocab CharBERT/data/dict/term_vocab \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --char_vocab CharBERT/data/dict/bert_char_vocab \\\n",
    "    --mlm_probability 0.10 \\\n",
    "    --input_nraws 1000 \\\n",
    "    --per_gpu_train_batch_size 4 \\\n",
    "    --per_gpu_eval_batch_size 4 \\\n",
    "    --save_steps 10000 \\\n",
    "    --block_size 384 \\\n",
    "    --mlm \\\n",
    "    --overwrite_output_dir "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run NER\n",
    "\n",
    "Pretrained Model: model/output/medical/mlm/base_bert_on_pubmed\n",
    "\n",
    "Dataset: conll2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|█| 18546/18546 [00:00<00:00, 1186925.89 ex\n",
      "Saving the dataset (1/1 shards): 100%|█| 1928/1928 [00:00<00:00, 182566.90 examp\n",
      "Saving the dataset (1/1 shards): 100%|█| 1928/1928 [00:00<00:00, 213705.55 examp\n"
     ]
    }
   ],
   "source": [
    "#model/output/medical/mlm/base_bert_on_pubmed\\\n",
    "! python datasets/load_medical_ner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-13 17:40:26.247888: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-13 17:40:26.250548: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-13 17:40:26.280040: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-13 17:40:26.280072: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-13 17:40:26.280900: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-13 17:40:26.285900: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-13 17:40:27.177746: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "02/13/2024 17:40:27 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "02/13/2024 17:40:28 - INFO - modeling.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/temp/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "02/13/2024 17:40:28 - INFO - modeling.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 9,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "02/13/2024 17:40:29 - INFO - modeling.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/temp/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
      "02/13/2024 17:40:30 - INFO - modeling.modeling_utils -   Weights of CharBertForTokenClassification not initialized from pretrained model: ['bert.char_embeddings.char_embeddings.weight', 'bert.char_embeddings.rnn_layer.weight_ih_l0', 'bert.char_embeddings.rnn_layer.weight_hh_l0', 'bert.char_embeddings.rnn_layer.bias_ih_l0', 'bert.char_embeddings.rnn_layer.bias_hh_l0', 'bert.char_embeddings.rnn_layer.weight_ih_l0_reverse', 'bert.char_embeddings.rnn_layer.weight_hh_l0_reverse', 'bert.char_embeddings.rnn_layer.bias_ih_l0_reverse', 'bert.char_embeddings.rnn_layer.bias_hh_l0_reverse', 'bert.encoder.word_linear1.weight', 'bert.encoder.char_linear1.weight', 'bert.encoder.fusion_layer_list.0.weight', 'bert.encoder.fusion_layer_list.0.bias', 'bert.encoder.fusion_layer_list.1.weight', 'bert.encoder.fusion_layer_list.1.bias', 'bert.encoder.fusion_layer_list.2.weight', 'bert.encoder.fusion_layer_list.2.bias', 'bert.encoder.fusion_layer_list.3.weight', 'bert.encoder.fusion_layer_list.3.bias', 'bert.encoder.fusion_layer_list.4.weight', 'bert.encoder.fusion_layer_list.4.bias', 'bert.encoder.fusion_layer_list.5.weight', 'bert.encoder.fusion_layer_list.5.bias', 'bert.encoder.fusion_layer_list.6.weight', 'bert.encoder.fusion_layer_list.6.bias', 'bert.encoder.fusion_layer_list.7.weight', 'bert.encoder.fusion_layer_list.7.bias', 'bert.encoder.fusion_layer_list.8.weight', 'bert.encoder.fusion_layer_list.8.bias', 'bert.encoder.fusion_layer_list.9.weight', 'bert.encoder.fusion_layer_list.9.bias', 'bert.encoder.fusion_layer_list.10.weight', 'bert.encoder.fusion_layer_list.10.bias', 'bert.encoder.fusion_layer_list.11.weight', 'bert.encoder.fusion_layer_list.11.bias', 'bert.encoder.word_norm.weight', 'bert.encoder.word_norm.bias', 'bert.encoder.char_norm.weight', 'bert.encoder.char_norm.bias', 'classifier.weight', 'classifier.bias']\n",
      "02/13/2024 17:40:30 - INFO - modeling.modeling_utils -   Weights from pretrained model not used in CharBertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "02/13/2024 17:40:32 - INFO - __main__ -   Training/evaluation parameters Namespace(data_dir='CharBERT/data/CoNLL2003/', model_type='bert', model_name_or_path='bert-base-cased', output_dir='model/output/medical/ner/base_bert_pretrained_on_pubmed/conll2003', char_vocab='CharBERT/data/dict/bert_char_vocab', labels='', config_name='', tokenizer_name='', cache_dir='', max_seq_length=128, do_train=True, do_eval=False, do_predict=True, evaluate_during_training=False, do_lower_case=False, per_gpu_train_batch_size=6, per_gpu_eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=3e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=1000, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'))\n",
      "02/13/2024 17:40:32 - INFO - __main__ -   Creating features from dataset file at CharBERT/data/CoNLL2003/\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   Writing example 0 of 14041\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   *** Example ***\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   guid: train-1\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   tokens: [CLS] EU rejects German call to boycott British la ##mb . [SEP]\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   char_ids: 0 0 44 54 0 7 1 58 1 12 2 8 0 49 1 7 14 3 5 0 12 3 10 10 0 2 6 0 20 6 19 12 6 2 2 0 35 7 4 2 4 8 9 0 10 3 0 14 20 0 23 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   start_ids: 0 2 5 13 20 25 28 36 44 47 50 52 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   end_ids: 0 3 11 18 23 26 34 42 45 48 50 52 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   input_ids: 101 7270 22961 1528 1840 1106 21423 1418 2495 12913 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   label_ids: -100 5 0 1 0 0 0 1 0 -100 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   *** Example ***\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   guid: train-2\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   tokens: [CLS] Peter Blackburn [SEP]\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   char_ids: 0 0 36 1 2 1 7 0 35 10 3 12 26 20 13 7 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   start_ids: 0 2 8 18 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   end_ids: 0 6 16 18 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   input_ids: 101 1943 14428 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   input_mask: 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   label_ids: -100 3 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   *** Example ***\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   guid: train-3\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   tokens: [CLS] BR ##US ##SE ##LS 1996 - 08 - 22 [SEP]\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   char_ids: 0 0 35 47 0 54 28 0 28 44 0 53 28 0 30 43 43 64 0 40 0 31 55 0 40 0 38 38 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   start_ids: 0 2 5 8 11 14 19 21 24 26 29 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   end_ids: 0 3 6 9 12 17 19 22 24 27 29 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   input_ids: 101 26660 13329 12649 15928 1820 118 4775 118 1659 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   label_ids: -100 7 -100 -100 -100 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   *** Example ***\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   guid: train-4\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   tokens: [CLS] The European Commission said on Thursday it disagreed with German advice to consumers to s ##hun British la ##mb until scientists determine whether mad cow disease can be transmitted to sheep . [SEP]\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   char_ids: 0 0 29 9 1 0 44 13 7 6 16 1 3 5 0 32 6 14 14 4 8 8 4 6 5 0 8 3 4 11 0 6 5 0 29 9 13 7 8 11 3 19 0 4 2 0 11 4 8 3 17 7 1 1 11 0 18 4 2 9 0 49 1 7 14 3 5 0 3 11 22 4 12 1 0 2 6 0 12 6 5 8 13 14 1 7 8 0 2 6 0 8 0 9 13 5 0 35 7 4 2 4 8 9 0 10 3 0 14 20 0 13 5 2 4 10 0 8 12 4 1 5 2 4 8 2 8 0 11 1 2 1 7 14 4 5 1 0 18 9 1 2 9 1 7 0 14 3 11 0 12 6 18 0 11 4 8 1 3 8 1 0 12 3 5 0 20 1 0 2 7 3 5 8 14 4 2 2 1 11 0 2 6 0 8 9 1 1 16 0 23 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   start_ids: 0 2 6 15 26 31 34 43 46 56 61 68 75 78 88 91 93 97 105 108 111 117 128 138 146 150 154 162 166 169 181 184 190 192 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   end_ids: 0 4 13 24 29 32 41 44 54 59 66 73 76 86 89 91 95 103 106 109 115 126 136 144 148 152 160 164 167 179 182 188 190 192 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   input_ids: 101 1109 1735 2827 1163 1113 9170 1122 19786 1114 1528 5566 1106 11060 1106 188 17315 1418 2495 12913 1235 6479 4959 2480 6340 13991 3653 1169 1129 12086 1106 8892 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   label_ids: -100 0 5 6 0 0 0 0 0 0 1 0 0 0 0 0 -100 1 0 -100 0 0 0 0 0 0 0 0 0 0 0 0 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   *** Example ***\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   guid: train-5\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   tokens: [CLS] Germany ' s representative to the European Union ' s veterinary committee Werner Z ##wing ##mann said on Wednesday consumers should buy sheep ##me ##at from countries other than Britain until the scientific advice was clearer . [SEP]\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   char_ids: 0 0 49 1 7 14 3 5 19 0 41 0 8 0 7 1 16 7 1 8 1 5 2 3 2 4 22 1 0 2 6 0 2 9 1 0 44 13 7 6 16 1 3 5 0 54 5 4 6 5 0 41 0 8 0 22 1 2 1 7 4 5 3 7 19 0 12 6 14 14 4 2 2 1 1 0 25 1 7 5 1 7 0 76 0 18 4 5 17 0 14 3 5 5 0 8 3 4 11 0 6 5 0 25 1 11 5 1 8 11 3 19 0 12 6 5 8 13 14 1 7 8 0 8 9 6 13 10 11 0 20 13 19 0 8 9 1 1 16 0 14 1 0 3 2 0 15 7 6 14 0 12 6 13 5 2 7 4 1 8 0 6 2 9 1 7 0 2 9 3 5 0 35 7 4 2 3 4 5 0 13 5 2 4 10 0 2 9 1 0 8 12 4 1 5 2 4 15 4 12 0 3 11 22 4 12 1 0 18 3 8 0 12 10 1 3 7 1 7 0 23 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   start_ids: 0 2 10 12 14 29 32 36 45 51 53 55 66 76 83 85 90 95 100 103 113 123 130 134 140 143 146 151 161 167 172 180 186 190 201 208 212 220 222 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   end_ids: 0 8 10 12 27 30 34 43 49 51 53 64 74 81 83 88 93 98 101 111 121 128 132 138 141 144 149 159 165 170 178 184 188 199 206 210 218 220 222 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767 767\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   input_ids: 101 1860 112 188 4702 1106 1103 1735 1913 112 188 27431 3914 14651 163 7635 4119 1163 1113 9031 11060 1431 4417 8892 3263 2980 1121 2182 1168 1190 2855 1235 1103 3812 5566 1108 27830 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/13/2024 17:40:32 - INFO - processors.utils_ner -   label_ids: -100 7 0 -100 0 0 0 5 6 0 -100 0 0 3 4 -100 -100 0 0 0 0 0 0 0 -100 -100 0 0 0 0 7 0 0 0 0 0 0 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "02/13/2024 17:40:35 - INFO - processors.utils_ner -   Writing example 10000 of 14041\n",
      "02/13/2024 17:40:36 - INFO - __main__ -   Saving features into cached file CharBERT/data/CoNLL2003/cached_train_bert-base-cased_128\n",
      "/home/temp/anaconda3/envs/flo_nlp/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "02/13/2024 17:40:40 - INFO - __main__ -   ***** Running training *****\n",
      "02/13/2024 17:40:40 - INFO - __main__ -     Num examples = 14041\n",
      "02/13/2024 17:40:40 - INFO - __main__ -     Num Epochs = 3\n",
      "02/13/2024 17:40:40 - INFO - __main__ -     Instantaneous batch size per GPU = 6\n",
      "02/13/2024 17:40:40 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "02/13/2024 17:40:40 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "02/13/2024 17:40:40 - INFO - __main__ -     Total optimization steps = 7023\n",
      "cls: <class 'modeling.configuration_bert.BertConfig'>\n",
      "pretrained_model_name_or_path: bert-base-cased\n",
      "pretrained_model_name_or_path: bert-base-cased\n",
      "cls.pretrained_config_archive_map: {'bert-base-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json', 'bert-large-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json', 'bert-base-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json', 'bert-large-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-config.json', 'bert-base-multilingual-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json', 'bert-base-multilingual-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json', 'bert-base-chinese': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json', 'bert-base-german-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-config.json', 'bert-large-uncased-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-config.json', 'bert-large-cased-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-config.json', 'bert-large-uncased-whole-word-masking-finetuned-squad': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-config.json', 'bert-large-cased-whole-word-masking-finetuned-squad': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-config.json', 'bert-base-cased-finetuned-mrpc': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-config.json', 'bert-base-german-dbmdz-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json', 'bert-base-german-dbmdz-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-uncased-config.json', 'bert-base-japanese': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-config.json', 'bert-base-japanese-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking-config.json', 'bert-base-japanese-char': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-char-config.json', 'bert-base-japanese-char-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-char-whole-word-masking-config.json', 'bert-base-finnish-cased-v1': 'https://s3.amazonaws.com/models.huggingface.co/bert/TurkuNLP/bert-base-finnish-cased-v1/config.json', 'bert-base-finnish-uncased-v1': 'https://s3.amazonaws.com/models.huggingface.co/bert/TurkuNLP/bert-base-finnish-uncased-v1/config.json'}\n",
      "Epoch:   0%|                                              | 0/3 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                       | 0/2341 [00:00<?, ?it/s]\u001b[A/home/temp/anaconda3/envs/flo_nlp/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "\n",
      "Iteration:   0%|                               | 1/2341 [00:00<30:54,  1.26it/s]\u001b[A\n",
      "Iteration:   0%|                               | 2/2341 [00:01<22:00,  1.77it/s]\u001b[A\n",
      "Iteration:   0%|                               | 3/2341 [00:01<19:24,  2.01it/s]\u001b[A\n",
      "Iteration:   0%|                               | 4/2341 [00:02<18:08,  2.15it/s]\u001b[A\n",
      "Iteration:   0%|                               | 5/2341 [00:02<17:29,  2.23it/s]\u001b[A\n",
      "Iteration:   0%|                               | 6/2341 [00:02<17:03,  2.28it/s]\u001b[A\n",
      "Iteration:   0%|                               | 7/2341 [00:03<16:47,  2.32it/s]\u001b[A\n",
      "Iteration:   0%|                               | 8/2341 [00:03<16:33,  2.35it/s]\u001b[A\n",
      "Iteration:   0%|                               | 9/2341 [00:04<16:27,  2.36it/s]\u001b[A\n",
      "Iteration:   0%|▏                             | 10/2341 [00:04<16:24,  2.37it/s]\u001b[A\n",
      "Iteration:   0%|▏                             | 11/2341 [00:04<16:17,  2.38it/s]\u001b[A\n",
      "Iteration:   1%|▏                             | 12/2341 [00:05<16:09,  2.40it/s]\u001b[A\n",
      "Iteration:   1%|▏                             | 13/2341 [00:05<16:06,  2.41it/s]\u001b[A\n",
      "Iteration:   1%|▏                             | 14/2341 [00:06<16:03,  2.42it/s]\u001b[A\n",
      "Iteration:   1%|▏                             | 15/2341 [00:06<15:59,  2.42it/s]\u001b[A\n",
      "Iteration:   1%|▏                             | 16/2341 [00:07<15:58,  2.43it/s]\u001b[A\n",
      "Iteration:   1%|▏                             | 17/2341 [00:07<15:55,  2.43it/s]\u001b[A\n",
      "Iteration:   1%|▏                             | 18/2341 [00:07<15:56,  2.43it/s]\u001b[A\n",
      "Iteration:   1%|▏                             | 19/2341 [00:08<15:51,  2.44it/s]\u001b[A\n",
      "Iteration:   1%|▎                             | 20/2341 [00:08<15:53,  2.43it/s]\u001b[A\n",
      "Iteration:   1%|▎                             | 21/2341 [00:09<15:52,  2.44it/s]\u001b[A\n",
      "Iteration:   1%|▎                             | 22/2341 [00:09<15:54,  2.43it/s]\u001b[A\n",
      "Iteration:   1%|▎                             | 23/2341 [00:09<15:55,  2.43it/s]\u001b[A\n",
      "Iteration:   1%|▎                             | 24/2341 [00:10<15:54,  2.43it/s]\u001b[A\n",
      "Iteration:   1%|▎                             | 25/2341 [00:10<15:53,  2.43it/s]\u001b[A\n",
      "Iteration:   1%|▎                             | 26/2341 [00:11<15:52,  2.43it/s]\u001b[A\n",
      "Iteration:   1%|▎                             | 27/2341 [00:11<15:56,  2.42it/s]\u001b[A\n",
      "Iteration:   1%|▎                             | 28/2341 [00:11<15:56,  2.42it/s]\u001b[A\n",
      "Iteration:   1%|▎                             | 29/2341 [00:12<15:53,  2.42it/s]\u001b[A\n",
      "Iteration:   1%|▍                             | 30/2341 [00:12<15:55,  2.42it/s]\u001b[A\n",
      "Iteration:   1%|▍                             | 31/2341 [00:13<15:53,  2.42it/s]\u001b[A\n",
      "Iteration:   1%|▍                             | 32/2341 [00:13<15:51,  2.43it/s]\u001b[A\n",
      "Iteration:   1%|▍                             | 33/2341 [00:14<15:51,  2.43it/s]\u001b[A\n",
      "Iteration:   1%|▍                             | 34/2341 [00:14<15:51,  2.42it/s]\u001b[A\n",
      "Iteration:   1%|▍                             | 35/2341 [00:14<15:50,  2.43it/s]\u001b[A\n",
      "Iteration:   2%|▍                             | 36/2341 [00:15<15:54,  2.41it/s]\u001b[A\n",
      "Iteration:   2%|▍                             | 37/2341 [00:15<15:53,  2.42it/s]\u001b[A\n",
      "Iteration:   2%|▍                             | 38/2341 [00:16<15:52,  2.42it/s]\u001b[A\n",
      "Iteration:   2%|▍                             | 39/2341 [00:16<15:54,  2.41it/s]\u001b[A\n",
      "Iteration:   2%|▌                             | 40/2341 [00:16<15:57,  2.40it/s]\u001b[A\n",
      "Iteration:   2%|▌                             | 41/2341 [00:17<15:57,  2.40it/s]\u001b[A\n",
      "Iteration:   2%|▌                             | 42/2341 [00:17<15:55,  2.41it/s]\u001b[A\n",
      "Iteration:   2%|▌                             | 43/2341 [00:18<15:52,  2.41it/s]\u001b[A\n",
      "Iteration:   2%|▌                             | 44/2341 [00:18<15:50,  2.42it/s]\u001b[A\n",
      "Iteration:   2%|▌                             | 45/2341 [00:18<15:51,  2.41it/s]\u001b[A\n",
      "Iteration:   2%|▌                             | 46/2341 [00:19<15:51,  2.41it/s]\u001b[A\n",
      "Iteration:   2%|▌                             | 47/2341 [00:19<15:51,  2.41it/s]\u001b[A\n",
      "Iteration:   2%|▌                             | 48/2341 [00:20<15:56,  2.40it/s]\u001b[A\n",
      "Iteration:   2%|▋                             | 49/2341 [00:20<15:55,  2.40it/s]\u001b[A^C\n",
      "Iteration:   2%|▋                             | 49/2341 [00:20<16:12,  2.36it/s]\n",
      "Epoch:   0%|                                              | 0/3 [00:20<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/temp/Documenti/flo_repos/nlp_charbert/CharBERT/run_ner.py\", line 549, in <module>\n",
      "    main()\n",
      "  File \"/home/temp/Documenti/flo_repos/nlp_charbert/CharBERT/run_ner.py\", line 479, in main\n",
      "    global_step, tr_loss = train(args, train_dataset, model, tokenizer, labels, pad_token_label_id)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/temp/Documenti/flo_repos/nlp_charbert/CharBERT/run_ner.py\", line 139, in train\n",
      "    outputs = model(**inputs)\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"/home/temp/anaconda3/envs/flo_nlp/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/temp/Documenti/flo_repos/nlp_charbert/CharBERT/modeling/modeling_charbert.py\", line 550, in forward\n",
      "    active_labels = labels.view(-1)[active_loss]\n",
      "                    ^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "! python CharBERT/run_ner.py --data_dir CharBERT/data/CoNLL2003/ \\\n",
    "                            --model_type bert \\\n",
    "                            --model_name_or_path bert-base-cased \\\n",
    "                            --output_dir model/output/medical/ner/base_bert_pretrained_on_pubmed/conll2003 \\\n",
    "                            --num_train_epochs 3 \\\n",
    "                            --learning_rate 3e-5 \\\n",
    "                            --char_vocab CharBERT/data/dict/bert_char_vocab \\\n",
    "                            --per_gpu_train_batch_size 6 \\\n",
    "                            --do_train \\\n",
    "                            --do_predict \\\n",
    "                            --overwrite_output_dir \\\n",
    "                            --save_steps 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run NER\n",
    "\n",
    "Pretrained Model: model/output/medical/mlm/base_bert_on_pubmed\n",
    "\n",
    "Dataset: jnlpba (biomedical-ner)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flo_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
