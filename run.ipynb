{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "\n",
    "#from datasets import load_dataset\n",
    "\n",
    "#ds = load_dataset(\"imdb\")\n",
    "#samples = ds[\"test\"][\"text\"][:100]\n",
    "\n",
    "#with open(\"eval_small.txt\", \"a\") as out:\n",
    "#    for s in samples:\n",
    "#        out.write(s + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements\n",
    "#import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUI\n",
      "device DEVICE DEVICE DEVICE DEVICE: cpu\n",
      "cls: <class 'modeling.configuration_bert.BertConfig'>\n",
      "pretrained_model_name_or_path: bert-base-cased\n",
      "pretrained_model_name_or_path: bert-base-cased\n",
      "cls.pretrained_config_archive_map: {'bert-base-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json', 'bert-large-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json', 'bert-base-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json', 'bert-large-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-config.json', 'bert-base-multilingual-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json', 'bert-base-multilingual-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json', 'bert-base-chinese': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json', 'bert-base-german-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-config.json', 'bert-large-uncased-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-config.json', 'bert-large-cased-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-config.json', 'bert-large-uncased-whole-word-masking-finetuned-squad': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-config.json', 'bert-large-cased-whole-word-masking-finetuned-squad': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-config.json', 'bert-base-cased-finetuned-mrpc': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-config.json', 'bert-base-german-dbmdz-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json', 'bert-base-german-dbmdz-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-uncased-config.json', 'bert-base-japanese': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-config.json', 'bert-base-japanese-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking-config.json', 'bert-base-japanese-char': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-char-config.json', 'bert-base-japanese-char-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-char-whole-word-masking-config.json', 'bert-base-finnish-cased-v1': 'https://s3.amazonaws.com/models.huggingface.co/bert/TurkuNLP/bert-base-finnish-cased-v1/config.json', 'bert-base-finnish-uncased-v1': 'https://s3.amazonaws.com/models.huggingface.co/bert/TurkuNLP/bert-base-finnish-uncased-v1/config.json'}\n",
      "file_path: Datasets/wikil_eng_train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/17/2024 15:34:15 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "01/17/2024 15:34:16 - INFO - modeling.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at C:\\Users\\claud\\.cache\\torch\\transformers\\b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "01/17/2024 15:34:16 - INFO - modeling.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "01/17/2024 15:34:17 - INFO - modeling.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at C:\\Users\\claud\\.cache\\torch\\transformers\\35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
      "01/17/2024 15:34:26 - INFO - modeling.modeling_utils -   Weights of CharBertForMaskedLM not initialized from pretrained model: ['bert.char_embeddings.char_embeddings.weight', 'bert.char_embeddings.rnn_layer.weight_ih_l0', 'bert.char_embeddings.rnn_layer.weight_hh_l0', 'bert.char_embeddings.rnn_layer.bias_ih_l0', 'bert.char_embeddings.rnn_layer.bias_hh_l0', 'bert.char_embeddings.rnn_layer.weight_ih_l0_reverse', 'bert.char_embeddings.rnn_layer.weight_hh_l0_reverse', 'bert.char_embeddings.rnn_layer.bias_ih_l0_reverse', 'bert.char_embeddings.rnn_layer.bias_hh_l0_reverse', 'bert.encoder.word_linear1.weight', 'bert.encoder.char_linear1.weight', 'bert.encoder.fusion_layer_list.0.weight', 'bert.encoder.fusion_layer_list.0.bias', 'bert.encoder.fusion_layer_list.1.weight', 'bert.encoder.fusion_layer_list.1.bias', 'bert.encoder.fusion_layer_list.2.weight', 'bert.encoder.fusion_layer_list.2.bias', 'bert.encoder.fusion_layer_list.3.weight', 'bert.encoder.fusion_layer_list.3.bias', 'bert.encoder.fusion_layer_list.4.weight', 'bert.encoder.fusion_layer_list.4.bias', 'bert.encoder.fusion_layer_list.5.weight', 'bert.encoder.fusion_layer_list.5.bias', 'bert.encoder.fusion_layer_list.6.weight', 'bert.encoder.fusion_layer_list.6.bias', 'bert.encoder.fusion_layer_list.7.weight', 'bert.encoder.fusion_layer_list.7.bias', 'bert.encoder.fusion_layer_list.8.weight', 'bert.encoder.fusion_layer_list.8.bias', 'bert.encoder.fusion_layer_list.9.weight', 'bert.encoder.fusion_layer_list.9.bias', 'bert.encoder.fusion_layer_list.10.weight', 'bert.encoder.fusion_layer_list.10.bias', 'bert.encoder.fusion_layer_list.11.weight', 'bert.encoder.fusion_layer_list.11.bias', 'bert.encoder.word_norm.weight', 'bert.encoder.word_norm.bias', 'bert.encoder.char_norm.weight', 'bert.encoder.char_norm.bias', 'cls.predictions.adv_transform.dense.weight', 'cls.predictions.adv_transform.dense.bias', 'cls.predictions.adv_transform.LayerNorm.weight', 'cls.predictions.adv_transform.LayerNorm.bias', 'cls.predictions.adv_decoder.weight', 'cls.predictions.adv_decoder.bias']\n",
      "01/17/2024 15:34:26 - INFO - modeling.modeling_utils -   Weights from pretrained model not used in CharBertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "01/17/2024 15:34:26 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='Datasets/wikil_eng_train.csv', output_dir='model/output/mlm/wikil_eng_train', eval_data_file='Datasets/wikil_eng_val.csv', char_vocab='CharBERT/data/dict/bert_char_vocab', term_vocab='CharBERT/data/dict/term_vocab', model_type='bert', model_name_or_path='bert-base-cased', mlm=True, mlm_probability=0.1, adv_probability=0.1, config_name='', data_version='', tokenizer_name='', cache_dir='', block_size=384, do_train=True, do_eval=True, output_debug=False, evaluate_during_training=False, do_lower_case=False, char_maxlen_for_word=6, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=4, gradient_accumulation_steps=1, learning_rate=3e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=10000, input_nraws=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=0, device=device(type='cpu'))\n",
      "C:\\Users\\claud\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "01/17/2024 15:34:27 - INFO - __main__ -   ***** Running training *****\n",
      "01/17/2024 15:34:27 - INFO - __main__ -     Num examples = 246652\n",
      "01/17/2024 15:34:27 - INFO - __main__ -     Num Epochs = 10\n",
      "01/17/2024 15:34:27 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
      "01/17/2024 15:34:27 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "01/17/2024 15:34:27 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "01/17/2024 15:34:27 - INFO - __main__ -     Total optimization steps = 616630\n",
      "\n",
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\n",
      "Iteration:   0%|          | 0/61663 [00:00<?, ?it/s]\u001b[ANone of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "\n",
      "Iteration:   0%|          | 0/61663 [00:13<?, ?it/s]\n",
      "\n",
      "Epoch:   0%|          | 0/10 [00:13<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\GitHub\\nlp_charbert\\CharBERT\\run_lm_finetuning.py\", line 968, in <module>\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"c:\\ProgramData\\miniconda3\\lib\\multiprocessing\\spawn.py\", line 116, in spawn_main\n",
      "    main()\n",
      "  File \"d:\\GitHub\\nlp_charbert\\CharBERT\\run_lm_finetuning.py\", line 920, in main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"c:\\ProgramData\\miniconda3\\lib\\multiprocessing\\spawn.py\", line 126, in _main\n",
      "    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
      "  File \"d:\\GitHub\\nlp_charbert\\CharBERT\\run_lm_finetuning.py\", line 519, in train\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "EOFError: Ran out of input\n",
      "    for step, batch in enumerate(epoch_iterator):\n",
      "  File \"c:\\ProgramData\\miniconda3\\lib\\site-packages\\tqdm\\std.py\", line 1178, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"C:\\Users\\claud\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py\", line 438, in __iter__\n",
      "    return self._get_iterator()\n",
      "  File \"C:\\Users\\claud\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py\", line 386, in _get_iterator\n",
      "    return _MultiProcessingDataLoaderIter(self)\n",
      "  File \"C:\\Users\\claud\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1039, in __init__\n",
      "    w.start()\n",
      "  File \"c:\\ProgramData\\miniconda3\\lib\\multiprocessing\\process.py\", line 121, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"c:\\ProgramData\\miniconda3\\lib\\multiprocessing\\context.py\", line 224, in _Popen\n",
      "    return _default_context.get_context().Process._Popen(process_obj)\n",
      "  File \"c:\\ProgramData\\miniconda3\\lib\\multiprocessing\\context.py\", line 336, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"c:\\ProgramData\\miniconda3\\lib\\multiprocessing\\popen_spawn_win32.py\", line 93, in __init__\n",
      "    reduction.dump(process_obj, to_child)\n",
      "  File \"c:\\ProgramData\\miniconda3\\lib\\multiprocessing\\reduction.py\", line 60, in dump\n",
      "    ForkingPickler(file, protocol).dump(obj)\n",
      "TypeError: cannot pickle '_io.TextIOWrapper' object\n"
     ]
    }
   ],
   "source": [
    "#!export CUDA_VISIBLE_DEVICES=1,2,3\n",
    "\n",
    "#pretrain charbert by bert_base_cased model\n",
    "#--char_vocab ./data/dict/roberta_char_vocab for robera\n",
    "\n",
    "!python CharBERT/run_lm_finetuning.py \\\n",
    "    --model_type bert \\\n",
    "    --model_name_or_path bert-base-cased \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_data_file Datasets/wikil_eng_train.csv \\\n",
    "    --eval_data_file  Datasets/wikil_eng_val.csv \\\n",
    "    --term_vocab CharBERT/data/dict/term_vocab \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --char_vocab CharBERT/data/dict/bert_char_vocab \\\n",
    "    --mlm_probability 0.10 \\\n",
    "    --input_nraws 1000 \\\n",
    "    --per_gpu_train_batch_size 4 \\\n",
    "    --per_gpu_eval_batch_size 4 \\\n",
    "    --save_steps 10000 \\\n",
    "    --block_size 384 \\\n",
    "    --mlm \\\n",
    "    --overwrite_output_dir \\\n",
    "    --output_dir model/output/mlm/wikil_eng_train\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flo_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
